{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwA1K1TNI5bsWcUIkhIgSZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b2d6cf479d542a28f332e2978b8204b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d5d118c76797450ea11dcd9459c0088e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8bd2e4ff5ec54927a550dc44285a33a1",
              "IPY_MODEL_1abff20776ea40b9b3f1a31f3c8447f3",
              "IPY_MODEL_91d8098897a74df2861dbedf76f178a4"
            ]
          }
        },
        "d5d118c76797450ea11dcd9459c0088e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8bd2e4ff5ec54927a550dc44285a33a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f80772bdf0294930872eea2f521736e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5238b219fe51482d81f6b8a86921fbea"
          }
        },
        "1abff20776ea40b9b3f1a31f3c8447f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d54b4a7c189b4a1397a177bfe822d1f3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5918af03bf384cba856ab3bfc5139082"
          }
        },
        "91d8098897a74df2861dbedf76f178a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0d044f310d754195ab83809dd6c3763c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 10.3kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb54ffe8f5d64a68a1b94ef7c036d948"
          }
        },
        "f80772bdf0294930872eea2f521736e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5238b219fe51482d81f6b8a86921fbea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d54b4a7c189b4a1397a177bfe822d1f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5918af03bf384cba856ab3bfc5139082": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d044f310d754195ab83809dd6c3763c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb54ffe8f5d64a68a1b94ef7c036d948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "39830252c4e84c849e3e9ff105ad772f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8a265553f4c14df7b9d9b7f33e9ebc76",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f5732462263748cf8e8c523ba71e3bd3",
              "IPY_MODEL_5bf4574dc5f24e9793a78d70ce57bbd2",
              "IPY_MODEL_077c4e85d0e743769e7811b84a6f8ef7"
            ]
          }
        },
        "8a265553f4c14df7b9d9b7f33e9ebc76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f5732462263748cf8e8c523ba71e3bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd4f9f8b275142249efbec848f713098",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fb2b87ff223b492db3551751582d8709"
          }
        },
        "5bf4574dc5f24e9793a78d70ce57bbd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a1d1f76bd98642d5bc6d9c9ad4a01ba0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4baeba2677be4152ba2d8ae30c6b4253"
          }
        },
        "077c4e85d0e743769e7811b84a6f8ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1334a00873b84796a4d28f1d8aab2a43",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 478M/478M [00:29&lt;00:00, 19.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_babd26e8000f4b019e297f8792ebf508"
          }
        },
        "bd4f9f8b275142249efbec848f713098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fb2b87ff223b492db3551751582d8709": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a1d1f76bd98642d5bc6d9c9ad4a01ba0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4baeba2677be4152ba2d8ae30c6b4253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1334a00873b84796a4d28f1d8aab2a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "babd26e8000f4b019e297f8792ebf508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "250adc1f5f194dbd96065cfa869ed1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4d5cda16398c421a9a22eaf9bc5ed60e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1f451c0afbcf4ee9ad4af100d0f168e7",
              "IPY_MODEL_7c2dbdabdac144ce99a26b531f799731",
              "IPY_MODEL_0009900c89bd41dcbe921829c6507fcf"
            ]
          }
        },
        "4d5cda16398c421a9a22eaf9bc5ed60e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1f451c0afbcf4ee9ad4af100d0f168e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f86f6c1cce2540f0aeb489106752147d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1f904835d034472497903b741a49ec62"
          }
        },
        "7c2dbdabdac144ce99a26b531f799731": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_528e1a2e18f9479baf0460fbed668e6c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c53223dbc284289a18d498db9d16e2d"
          }
        },
        "0009900c89bd41dcbe921829c6507fcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32cd0cef48444803b49b6680aff760f0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 878k/878k [00:00&lt;00:00, 2.63MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9e58a6cf64f480ebdd1a18127cab5e0"
          }
        },
        "f86f6c1cce2540f0aeb489106752147d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1f904835d034472497903b741a49ec62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "528e1a2e18f9479baf0460fbed668e6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c53223dbc284289a18d498db9d16e2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32cd0cef48444803b49b6680aff760f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9e58a6cf64f480ebdd1a18127cab5e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a43b5fa66321439eaa7a051205b51c93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d5e6e12511f74ef8ad7772eb96d35439",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a82beeb6d6804d22902ea83bdd12e8f7",
              "IPY_MODEL_d3783d328fe145e38059c418456e0f58",
              "IPY_MODEL_b0e1cf3a1c5b479ea67aed5176548601"
            ]
          }
        },
        "d5e6e12511f74ef8ad7772eb96d35439": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a82beeb6d6804d22902ea83bdd12e8f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_671bb89065ad42029641079f604bc810",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b22dec0da5a4c4c8e7d56a1b7919128"
          }
        },
        "d3783d328fe145e38059c418456e0f58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_12145d14186945999214a675d2c3ab3d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_788f916ba0d144dd80258c48d4c36134"
          }
        },
        "b0e1cf3a1c5b479ea67aed5176548601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_607cd98f58934ef082cd916c8ea41ea1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 1.93MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47e6636aa6634ac881f19fb872a9eb55"
          }
        },
        "671bb89065ad42029641079f604bc810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b22dec0da5a4c4c8e7d56a1b7919128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "12145d14186945999214a675d2c3ab3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "788f916ba0d144dd80258c48d4c36134": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "607cd98f58934ef082cd916c8ea41ea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47e6636aa6634ac881f19fb872a9eb55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dea944d3b6b9439991e891fc0b32f351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c271517570fa4070b988e48d4a4ef3ce",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_27395344c2704155871f94d991bebabb",
              "IPY_MODEL_30dffb37cf6f4d3a878a6027ddcc75d5",
              "IPY_MODEL_1d46e12d5c044b2382116676362c39f6"
            ]
          }
        },
        "c271517570fa4070b988e48d4a4ef3ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "27395344c2704155871f94d991bebabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b423ad1dfd6a4c02a15a075ba5925794",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6dfca3d6102450db5bcef1e58b88b9f"
          }
        },
        "30dffb37cf6f4d3a878a6027ddcc75d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_971e0f9f76a04978a8d459bfece996cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5503127e86544c88c7899588d6e2477"
          }
        },
        "1d46e12d5c044b2382116676362c39f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fffe234d6ad646209cfa470e681f441b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 4.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b79b936152054537ae04d0d228395790"
          }
        },
        "b423ad1dfd6a4c02a15a075ba5925794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6dfca3d6102450db5bcef1e58b88b9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "971e0f9f76a04978a8d459bfece996cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5503127e86544c88c7899588d6e2477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fffe234d6ad646209cfa470e681f441b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b79b936152054537ae04d0d228395790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2b7cc062952543ec8611eacd39cbf56b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_38bc60da98c444dc91e0cdc735443455",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_98e1010942fd4b7bbb11047c4c34807c",
              "IPY_MODEL_80bc3dac02014f6ea8bd28a816be464e",
              "IPY_MODEL_375dd6d0daa64f11acb148ae64f7ea84"
            ]
          }
        },
        "38bc60da98c444dc91e0cdc735443455": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98e1010942fd4b7bbb11047c4c34807c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_96859ef96dce470d903dd0f5db492e7f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1aa98bc73b24af1becabb09dce02a3a"
          }
        },
        "80bc3dac02014f6ea8bd28a816be464e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4b4180d49bb34d56ac94116313fdfbbe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 481,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 481,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e200499c91324be7b6bedce0b399114a"
          }
        },
        "375dd6d0daa64f11acb148ae64f7ea84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_554d5ecda75d49129471c02b03d8fd58",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 481/481 [00:00&lt;00:00, 12.4kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8b58b2bf1414c3b914ea6510468a2b0"
          }
        },
        "96859ef96dce470d903dd0f5db492e7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1aa98bc73b24af1becabb09dce02a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b4180d49bb34d56ac94116313fdfbbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e200499c91324be7b6bedce0b399114a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "554d5ecda75d49129471c02b03d8fd58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8b58b2bf1414c3b914ea6510468a2b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a65aceccb14d496ab4c14e8511132a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_985b30774b62407ab7ea1be287f91f09",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7c85dfdce42f4a8fb731932d402c1804",
              "IPY_MODEL_0c708454f17a4b00aebe5c9355e9fca5",
              "IPY_MODEL_12d9e28c0ffb4d829f40c64348d78abf"
            ]
          }
        },
        "985b30774b62407ab7ea1be287f91f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7c85dfdce42f4a8fb731932d402c1804": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_250f09a7fc20402c800828f7c6c3c81a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5226a07a55a842939c9a227444c70354"
          }
        },
        "0c708454f17a4b00aebe5c9355e9fca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_585f3e5d77ad4c1a86904d2fc0d79187",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 501200538,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 501200538,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7537b1fc3ab243a68a13cdbbdedf36fa"
          }
        },
        "12d9e28c0ffb4d829f40c64348d78abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f14a8b58157444578f4eb5c4c26458d2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 478M/478M [00:09&lt;00:00, 56.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c56d2b1eac9044e8b5656e7d2393c920"
          }
        },
        "250f09a7fc20402c800828f7c6c3c81a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5226a07a55a842939c9a227444c70354": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "585f3e5d77ad4c1a86904d2fc0d79187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7537b1fc3ab243a68a13cdbbdedf36fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f14a8b58157444578f4eb5c4c26458d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c56d2b1eac9044e8b5656e7d2393c920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5d31186deed4ab8b23b6a8af8a184e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_201a703257dc4cd492bec5fffb014e95",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5416d599e1c4331ae8b12fec5d2d52d",
              "IPY_MODEL_7139941e7b1248458b27c22b0965e0d7",
              "IPY_MODEL_ecb98863df6549de82dc27c68c749c23"
            ]
          }
        },
        "201a703257dc4cd492bec5fffb014e95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c5416d599e1c4331ae8b12fec5d2d52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d53dbbea1cc148a7bc106bf63e792178",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_475f62f4f5f84901921c3bf2681aca5b"
          }
        },
        "7139941e7b1248458b27c22b0965e0d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a086442e0a464025a48dad1df9d582e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 898823,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 898823,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b709aa99cb914ed283a004ea910d68f3"
          }
        },
        "ecb98863df6549de82dc27c68c749c23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0ce83c1fb3044f688b9baca4bcb1087e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 878k/878k [00:00&lt;00:00, 1.95MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aa35a5d2eb3744479bb1a95966bd5bb8"
          }
        },
        "d53dbbea1cc148a7bc106bf63e792178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "475f62f4f5f84901921c3bf2681aca5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a086442e0a464025a48dad1df9d582e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b709aa99cb914ed283a004ea910d68f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0ce83c1fb3044f688b9baca4bcb1087e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aa35a5d2eb3744479bb1a95966bd5bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ea75a0f6c2149fabe685155a9343ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fd47bb41646a4d3298163cb7f1af62dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_32523362d7914aca9751e19689d47978",
              "IPY_MODEL_22fbcc9e89eb4513bdad988a45ae9e43",
              "IPY_MODEL_a83670cf22bb4c77986b9b26befb3553"
            ]
          }
        },
        "fd47bb41646a4d3298163cb7f1af62dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32523362d7914aca9751e19689d47978": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_229b5131275a47e3b960e4ed211b0ffb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1e1d7c7e1854333bde9d273e48c5135"
          }
        },
        "22fbcc9e89eb4513bdad988a45ae9e43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e1014cc980f54f45b766a16ef080a360",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f3595b2d2c34211ad35379c13b6d600"
          }
        },
        "a83670cf22bb4c77986b9b26befb3553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d91db9338acf492cb8d02ed2f93f7744",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 446k/446k [00:00&lt;00:00, 582kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_74082303c43c40e38a9fdd5b6a342e03"
          }
        },
        "229b5131275a47e3b960e4ed211b0ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1e1d7c7e1854333bde9d273e48c5135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1014cc980f54f45b766a16ef080a360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f3595b2d2c34211ad35379c13b6d600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d91db9338acf492cb8d02ed2f93f7744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "74082303c43c40e38a9fdd5b6a342e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f44176b1b27345f99c4b7f4d83519092": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_40ef10c013c04e4b9e2af6d91285114b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1b28e92252142da9100bea28d88b652",
              "IPY_MODEL_aa6d7bd1db334c55b34e35a5efd5d212",
              "IPY_MODEL_7ebb6a61a15145439849ea63c7189640"
            ]
          }
        },
        "40ef10c013c04e4b9e2af6d91285114b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1b28e92252142da9100bea28d88b652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fde5b2c3daa74152bc8c127428c40e3f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba1a1c8181204129b8396ac9bcee153e"
          }
        },
        "aa6d7bd1db334c55b34e35a5efd5d212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5f6b9e12cba44381b1fd772d77461e1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355863,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355863,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_490e73510f634d5e909a66a8b16f68ad"
          }
        },
        "7ebb6a61a15145439849ea63c7189640": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e4e5759e0e944a9a8b8f8c012818d507",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 2.23MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_714585bd7dd64c2d9eb1d069c56940fd"
          }
        },
        "fde5b2c3daa74152bc8c127428c40e3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba1a1c8181204129b8396ac9bcee153e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f6b9e12cba44381b1fd772d77461e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "490e73510f634d5e909a66a8b16f68ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4e5759e0e944a9a8b8f8c012818d507": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "714585bd7dd64c2d9eb1d069c56940fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b09f51448a4b46489093626eee3b829d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_493cc8a77b33484d8b1ce58f56fa0e40",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4c91ad4695bf4ad0b39ea727cd4234bd",
              "IPY_MODEL_42c9423d9cdc4a63831113eb7beb0fb2",
              "IPY_MODEL_a4cf3d667beb45cca2e79e12752809ee"
            ]
          }
        },
        "493cc8a77b33484d8b1ce58f56fa0e40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4c91ad4695bf4ad0b39ea727cd4234bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_92c0ae7a32cf491baa6994ca0838195e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc5a9a7bd33d476d99695b4708cb5d9c"
          }
        },
        "42c9423d9cdc4a63831113eb7beb0fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c572724f886f4d5f973e901efacece11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fdb1c71ed6b846d3aa397634b56a7300"
          }
        },
        "a4cf3d667beb45cca2e79e12752809ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1e5840b5d1fd4810912466af9a86d6e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 17.2kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fbb516b92b744bf78ed7b4e5505ac7cd"
          }
        },
        "92c0ae7a32cf491baa6994ca0838195e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc5a9a7bd33d476d99695b4708cb5d9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c572724f886f4d5f973e901efacece11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fdb1c71ed6b846d3aa397634b56a7300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1e5840b5d1fd4810912466af9a86d6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fbb516b92b744bf78ed7b4e5505ac7cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cd4b06ad2e6419399bcfaed46dca250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce280a52c7de41dc84cab45ef4ff2bc0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_407cff14b3694c61bcaffb90394cb683",
              "IPY_MODEL_80ba9119f80b4e32a366f037c3a97fcb",
              "IPY_MODEL_cbeea8516f64426b9295f937a75cd3a2"
            ]
          }
        },
        "ce280a52c7de41dc84cab45ef4ff2bc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "407cff14b3694c61bcaffb90394cb683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7765f5748f024cdd9da32ef581a21aa7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9006be48038f49adaeff1006e07a1892"
          }
        },
        "80ba9119f80b4e32a366f037c3a97fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cbb9df6b767b4cff8412ad3bcf3d67d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0676b8c819e34d79ac55dc5b332ad2b5"
          }
        },
        "cbeea8516f64426b9295f937a75cd3a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dcbb5a05251148c889b6200e1e8bcaf8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 420M/420M [00:31&lt;00:00, 12.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_296598f76282494d8f19c266973298bf"
          }
        },
        "7765f5748f024cdd9da32ef581a21aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9006be48038f49adaeff1006e07a1892": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cbb9df6b767b4cff8412ad3bcf3d67d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0676b8c819e34d79ac55dc5b332ad2b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dcbb5a05251148c889b6200e1e8bcaf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "296598f76282494d8f19c266973298bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38136e3616e14b5fb8a5cd506f31dc13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7368e19be953492a853aa062eb78c6a2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0022b4489c5f457da646ca338c4b3177",
              "IPY_MODEL_fa1411dcc5a447129bc30541cc543936",
              "IPY_MODEL_fc88b6e6638449699ea6f1b8047a0855"
            ]
          }
        },
        "7368e19be953492a853aa062eb78c6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0022b4489c5f457da646ca338c4b3177": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95d4eb1b0b6f481e818fddc29ce598ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_98259c9325f44f67a6e89ac4abe9b193"
          }
        },
        "fa1411dcc5a447129bc30541cc543936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_55ce51f2ce294a13bdb98a63f02f4a56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_733827eaaa3f47a8a03a7e140cd98c14"
          }
        },
        "fc88b6e6638449699ea6f1b8047a0855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aa6fd0e779344cb1a4922a59aa1f8124",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 581kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7175f082dea540f6832971ead2899e24"
          }
        },
        "95d4eb1b0b6f481e818fddc29ce598ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "98259c9325f44f67a6e89ac4abe9b193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55ce51f2ce294a13bdb98a63f02f4a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "733827eaaa3f47a8a03a7e140cd98c14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa6fd0e779344cb1a4922a59aa1f8124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7175f082dea540f6832971ead2899e24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e3e60f192fb44882ac170d37bbd0b891": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8bca92d880cd42708652d91a1dc58c28",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37732d263b1e43838c814ce7523b291e",
              "IPY_MODEL_17626195152a40f0bec6b0751ee5f130",
              "IPY_MODEL_a29b4243e43c4e4ca7665b1b1976220d"
            ]
          }
        },
        "8bca92d880cd42708652d91a1dc58c28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37732d263b1e43838c814ce7523b291e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45ee0f7b00fb4fe9a69acc575a0665dd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_58d0d0f8f84c4ba5a95b9ad79b2851e6"
          }
        },
        "17626195152a40f0bec6b0751ee5f130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f6932a37e0724c12972f21983a7ba748",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_204507984b334521bae37a7d56c113ac"
          }
        },
        "a29b4243e43c4e4ca7665b1b1976220d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9cecaa54ca2f4887992c58f592d5dd60",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 850B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_476774eaaafd463ba77721560e532025"
          }
        },
        "45ee0f7b00fb4fe9a69acc575a0665dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "58d0d0f8f84c4ba5a95b9ad79b2851e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f6932a37e0724c12972f21983a7ba748": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "204507984b334521bae37a7d56c113ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9cecaa54ca2f4887992c58f592d5dd60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "476774eaaafd463ba77721560e532025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b551a69c89e54ae7b0270f4a9ab132bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61f71bd185fc46718f6e791809b6f0f0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e5dbfc13dfea4257a876d1c68ade3287",
              "IPY_MODEL_dd1692edb5354bbc91a36ed70cff4996",
              "IPY_MODEL_4813c01f9ea34ac6a85f046d4d59c606"
            ]
          }
        },
        "61f71bd185fc46718f6e791809b6f0f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e5dbfc13dfea4257a876d1c68ade3287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_915a080687cb4c96865990d26a820aa0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_40e0dde29b6f4d8dbd55562ab4821f6d"
          }
        },
        "dd1692edb5354bbc91a36ed70cff4996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3b07f9a0ce334646b47f695ca0e8b684",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_036c80b6d0e74b28a3e385d72ceb4fa8"
          }
        },
        "4813c01f9ea34ac6a85f046d4d59c606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_bd99a75d4f114c22a26f6f98aef190e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 455k/455k [00:00&lt;00:00, 717kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1619d3792bf432a8c8c8ea7fac2a3cb"
          }
        },
        "915a080687cb4c96865990d26a820aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "40e0dde29b6f4d8dbd55562ab4821f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b07f9a0ce334646b47f695ca0e8b684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "036c80b6d0e74b28a3e385d72ceb4fa8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bd99a75d4f114c22a26f6f98aef190e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1619d3792bf432a8c8c8ea7fac2a3cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d68d583e3e08432a8665418827557194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a3ca5644a9014cf084aa4baa38f6b2f3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_44453920c8c74ee096e16f9be1e83637",
              "IPY_MODEL_64e603c1577d4bb7a0e8f1df7aeedf64",
              "IPY_MODEL_0c575190e3bd4d2db41e9fc0f46c282e"
            ]
          }
        },
        "a3ca5644a9014cf084aa4baa38f6b2f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44453920c8c74ee096e16f9be1e83637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e0f2b4d64b584515b9ee012afb631776",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aadc1f0bbc8048f28489c599c0cb741c"
          }
        },
        "64e603c1577d4bb7a0e8f1df7aeedf64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3a7e3f6a87394b58990a46a18af6237a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 421,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 421,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f9e4bb4f95294fc3a508a9db626f0708"
          }
        },
        "0c575190e3bd4d2db41e9fc0f46c282e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6ac801aac05948d2866f64e3f2640a6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 421/421 [00:00&lt;00:00, 12.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b091d5c6016d4c2f8ce34fa1d6cdecdd"
          }
        },
        "e0f2b4d64b584515b9ee012afb631776": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aadc1f0bbc8048f28489c599c0cb741c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a7e3f6a87394b58990a46a18af6237a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f9e4bb4f95294fc3a508a9db626f0708": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6ac801aac05948d2866f64e3f2640a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b091d5c6016d4c2f8ce34fa1d6cdecdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00288cce28f84ed9bb00f11795478d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_abcd4f8492cd4efcad3c9314a9738722",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cb24b20bee31485da8fa2afc30e561e2",
              "IPY_MODEL_a18508a9157141fcbc867c718e9ded1b",
              "IPY_MODEL_b4daa67006544f399c31b3feb1b3da3d"
            ]
          }
        },
        "abcd4f8492cd4efcad3c9314a9738722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cb24b20bee31485da8fa2afc30e561e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f1b2b8ae3d584408a7bcef36b5b564cc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_430e741f529a4bf381591a66afb1d7fb"
          }
        },
        "a18508a9157141fcbc867c718e9ded1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e08cdc345f984d579fc0b28415999671",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1345000672,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1345000672,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b85dc7fceaf2437aadf5dc2091008939"
          }
        },
        "b4daa67006544f399c31b3feb1b3da3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f6fe31cc4f447cc8f51f12018ae2620",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.25G/1.25G [01:19&lt;00:00, 12.1MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_995c5fceee2d4e02a2084ae699596ae8"
          }
        },
        "f1b2b8ae3d584408a7bcef36b5b564cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "430e741f529a4bf381591a66afb1d7fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e08cdc345f984d579fc0b28415999671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b85dc7fceaf2437aadf5dc2091008939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f6fe31cc4f447cc8f51f12018ae2620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "995c5fceee2d4e02a2084ae699596ae8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89fa44bdf1f44b629847e82433b18d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a89e0fc32724d009baa9b6da896bcf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3513d4ccf7724d549374dc4c8417496c",
              "IPY_MODEL_b2e496797e504f04b5612586ff9d71c0",
              "IPY_MODEL_406d4c7928254b0b985ffca0c0aa1de2"
            ]
          }
        },
        "5a89e0fc32724d009baa9b6da896bcf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3513d4ccf7724d549374dc4c8417496c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1b3ffe85beb347efafc7a0c674ac3c9b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6742a23fd305470687e96fdb4ebc75cc"
          }
        },
        "b2e496797e504f04b5612586ff9d71c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9588a858e5624a008b911ef2d33b798e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a0b201a760844f76bd939ea3700327a6"
          }
        },
        "406d4c7928254b0b985ffca0c0aa1de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fe393216b80c4de6b5993ecd1ec26c79",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 226k/226k [00:00&lt;00:00, 599kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30c7b73987624a8ba0914a8f801edc25"
          }
        },
        "1b3ffe85beb347efafc7a0c674ac3c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6742a23fd305470687e96fdb4ebc75cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9588a858e5624a008b911ef2d33b798e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a0b201a760844f76bd939ea3700327a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe393216b80c4de6b5993ecd1ec26c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30c7b73987624a8ba0914a8f801edc25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shalini61298/Twitter_Analysis/blob/main/phaseFULL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk_r8P-F3S99"
      },
      "source": [
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils import data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import ( AdamW, BertModel, RobertaModel)\n",
        "from transformers import BertTokenizer, RobertaTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy\n",
        "import nltk\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import argparse\n",
        "import gc\n",
        "import sys\n",
        "import os\n",
        "from nltk import word_tokenize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T2lDe993XM5",
        "outputId": "e5dcf5e1-dd8c-45a0-b76e-d4f8384cfa82"
      },
      "source": [
        "# Stemming\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "# Stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2b7cc062952543ec8611eacd39cbf56b",
            "38bc60da98c444dc91e0cdc735443455",
            "98e1010942fd4b7bbb11047c4c34807c",
            "80bc3dac02014f6ea8bd28a816be464e",
            "375dd6d0daa64f11acb148ae64f7ea84",
            "96859ef96dce470d903dd0f5db492e7f",
            "b1aa98bc73b24af1becabb09dce02a3a",
            "4b4180d49bb34d56ac94116313fdfbbe",
            "e200499c91324be7b6bedce0b399114a",
            "554d5ecda75d49129471c02b03d8fd58",
            "c8b58b2bf1414c3b914ea6510468a2b0",
            "a65aceccb14d496ab4c14e8511132a36",
            "985b30774b62407ab7ea1be287f91f09",
            "7c85dfdce42f4a8fb731932d402c1804",
            "0c708454f17a4b00aebe5c9355e9fca5",
            "12d9e28c0ffb4d829f40c64348d78abf",
            "250f09a7fc20402c800828f7c6c3c81a",
            "5226a07a55a842939c9a227444c70354",
            "585f3e5d77ad4c1a86904d2fc0d79187",
            "7537b1fc3ab243a68a13cdbbdedf36fa",
            "f14a8b58157444578f4eb5c4c26458d2",
            "c56d2b1eac9044e8b5656e7d2393c920",
            "c5d31186deed4ab8b23b6a8af8a184e0",
            "201a703257dc4cd492bec5fffb014e95",
            "c5416d599e1c4331ae8b12fec5d2d52d",
            "7139941e7b1248458b27c22b0965e0d7",
            "ecb98863df6549de82dc27c68c749c23",
            "d53dbbea1cc148a7bc106bf63e792178",
            "475f62f4f5f84901921c3bf2681aca5b",
            "a086442e0a464025a48dad1df9d582e3",
            "b709aa99cb914ed283a004ea910d68f3",
            "0ce83c1fb3044f688b9baca4bcb1087e",
            "aa35a5d2eb3744479bb1a95966bd5bb8",
            "8ea75a0f6c2149fabe685155a9343ecf",
            "fd47bb41646a4d3298163cb7f1af62dd",
            "32523362d7914aca9751e19689d47978",
            "22fbcc9e89eb4513bdad988a45ae9e43",
            "a83670cf22bb4c77986b9b26befb3553",
            "229b5131275a47e3b960e4ed211b0ffb",
            "b1e1d7c7e1854333bde9d273e48c5135",
            "e1014cc980f54f45b766a16ef080a360",
            "8f3595b2d2c34211ad35379c13b6d600",
            "d91db9338acf492cb8d02ed2f93f7744",
            "74082303c43c40e38a9fdd5b6a342e03",
            "f44176b1b27345f99c4b7f4d83519092",
            "40ef10c013c04e4b9e2af6d91285114b",
            "c1b28e92252142da9100bea28d88b652",
            "aa6d7bd1db334c55b34e35a5efd5d212",
            "7ebb6a61a15145439849ea63c7189640",
            "fde5b2c3daa74152bc8c127428c40e3f",
            "ba1a1c8181204129b8396ac9bcee153e",
            "5f6b9e12cba44381b1fd772d77461e1b",
            "490e73510f634d5e909a66a8b16f68ad",
            "e4e5759e0e944a9a8b8f8c012818d507",
            "714585bd7dd64c2d9eb1d069c56940fd"
          ]
        },
        "id": "gkhKFvAG3aKy",
        "outputId": "0a46cf92-1884-4207-91e7-d98d2549aa24"
      },
      "source": [
        "\n",
        "def string_to_bool(string):\n",
        "\n",
        "    if string.lower() == 'false':\n",
        "        return False\n",
        "    elif string.lower() == 'true':\n",
        "        return True\n",
        "    else:\n",
        "        return ValueError\n",
        "\n",
        "\n",
        "def pre_process(tweet):\n",
        "\n",
        "    tweet = re.sub(\"HTTPURL\", \"http\", tweet)\n",
        "    tweet = \" \".join([\" \".join([word for word in re.sub('([A-Z][a-z]+)', r' \\1',\n",
        "                                                        re.sub('([A-Z]+)', r' \\1', word)).split()]) if word.startswith(\n",
        "        '#') else word for word in tweet.split()])\n",
        "    tweet = re.sub('# ', '#', tweet)\n",
        "\n",
        "    # convert instances of covid/corona to 'virus' for which there is an embedding\n",
        "    tweet = re.sub('(?i)CORONA$', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID-19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVIDー19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirusー19', 'coronavirus', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_emojis(data):\n",
        "\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "\n",
        "def handcrafted_feature(text):\n",
        "\n",
        "    text = remove_emojis(text)\n",
        "    text = [char for char in text if char not in string.punctuation]\n",
        "    count = len([char for char in text if char.isdigit()])\n",
        "    avg = (count/(len(text) - text.count(\" \")))*100\n",
        "    x =  torch.tensor([avg], dtype=torch.float)\n",
        "    y = x.view(1,1)\n",
        "    return y\n",
        "\n",
        "\n",
        "config     = {\"model_class\": \"RoBERTa\",\n",
        "\"model\" : \"roberta-base\",\n",
        "\"random_seed\": int(\"4\"),\n",
        "\"max_len\": int(\"128\"),\n",
        "\"epochs\": int(\"4\"),\n",
        "\"learning_rate\": float(\"1e-5\"),\n",
        "\"batch_size\": int(\"32\"),\n",
        "\"dropout_prob\": float(\"0.2\"),\n",
        "\"test_size\": float(\"0.3\"),\n",
        "\"preprocessed\": string_to_bool(\"True\"),\n",
        "\"ensemble\": string_to_bool(\"True\"),\n",
        "\"num_labels\": int(\"2\"),\n",
        "\"max_feats\": int(\"6000\"),\n",
        "\"save_model\": string_to_bool(\"False\"),\n",
        "\"split_train\": string_to_bool(\"False\")}\n",
        "\n",
        "\n",
        "\n",
        "train_data = None\n",
        "df_val     = None\n",
        "\n",
        "MODEL         = None\n",
        "RANDOM_SEED   = None\n",
        "MAX_LEN       = None\n",
        "BATCH_SIZE    = None\n",
        "EPOCHS        = None\n",
        "LEARNING_RATE = None\n",
        "TEST_SIZE     = None\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  train_data = pd.read_csv('train.tsv', sep=r'\\t', header=0)\n",
        "  #assert train_data.shape == (69, 3) , \"train.tsv should contain 7000 rows, only found \" + str(train_data.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/train.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "try:\n",
        "  df_val = pd.read_csv('fakenewsoutput.tsv', sep=r'\\t', header=0)\n",
        "\n",
        "  #assert df_val.shape == (9, 3) , \"valid.tsv should contain 1000 rows, only found \" + str(df_val.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/valid.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to train text and validation text\n",
        "if config['preprocessed']:\n",
        "  train_data.Text = train_data.Text.apply(lambda x: pre_process(x))\n",
        "  #print(df_val)\n",
        "  df_val.Text     = df_val.Text.apply(lambda x: pre_process(x))\n",
        "\n",
        "# Currently the labels in our dataframes are \"UNINFORMATIVE\" or \"INFORMATIVE\"\n",
        "train_data.Label  = train_data.Label.apply(lambda x: 0 if x == 'UNINFORMATIVE' else 1)\n",
        "df_val.Label      = df_val.Label.apply(lambda x: 0 if x=='UNINFORMATIVE' else 1)\n",
        "\n",
        "\n",
        "# Configure Model\n",
        "\n",
        "TRANSFORMERS = {\n",
        "    \"BERT\" : (BertModel, BertTokenizer),\n",
        "    \"RoBERTa\": (RobertaModel, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "transformer_class, tokenizer_class = TRANSFORMERS.get(config[\"model_class\"])\n",
        "\n",
        "# Define model\n",
        "transformer_model = transformer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer   = tokenizer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define sequence length, batch size and random state\n",
        "MAX_LEN     = config['max_len']\n",
        "BATCH_SIZE  = config['batch_size']\n",
        "RANDOM_SEED = config['random_seed']\n",
        "TEST_SIZE   = config['test_size']\n",
        "ENSEMBLE    = config['ensemble']\n",
        "MAX_FEATS   = config['max_feats']\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "  torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text, label, tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "        'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "if config['split_train']: # split training to create a test set\n",
        "  df_train, df_test = train_test_split(train_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "  test_dataloader   = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "train_dataloader = (create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) if config['split_train']\n",
        "                    else create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE))\n",
        "val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = \"\".join([char for char in text if not char.isdigit()])\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "if ENSEMBLE:\n",
        "\n",
        "  # Instantiate TfidfVectorizer object and pass clean_text method which it will automatically apply\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text, max_features=MAX_FEATS)\n",
        "\n",
        "  # Learning vocabulary on training data\n",
        "  if config['split_train']:\n",
        "        tfidf_vect.fit(df_train.Text)\n",
        "  else:\n",
        "        tfidf_vect.fit(train_data.Text)\n",
        "\n",
        "# Model Classes\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(RobertaClassificationHead, self).__init__()\n",
        "    self.input_size  = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels  = num_labels\n",
        "    self.dropout     = nn.Dropout(p=dropout_prob)\n",
        "    self.activation  = nn.Tanh()\n",
        "    #self.dense      = nn.Linear(hidden_size , hidden_size) # Note - atm hidden_size\n",
        "    self.dense      = nn.Linear(self.input_size , self.input_size) # Note - atm hidden_size\n",
        "    self.out_proj   = nn.Linear(self.input_size , self.num_labels)\n",
        "\n",
        "\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dropout(x) # Roberta has extra dropout layer, and experimentation showed this gets better results.\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BertClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(BertClassificationHead, self).__init__()\n",
        "    self.input_size = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels = num_labels\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dense      = nn.Linear(self.input_size, self.input_size)\n",
        "    self.dropout    = nn.Dropout(p=dropout_prob)\n",
        "    self.out_proj   = nn.Linear(self.input_size, self.num_labels)\n",
        "\n",
        "\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take [CLS]\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, model, num_labels, classification_head):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.num_labels = num_labels\n",
        "    self.classification_head = classification_head\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None,\n",
        "              ensemble=None, tfidf_feats=None, avg_feats=None):\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    model_output = outputs[0]\n",
        "    logits = self.classification_head(model_output, ensemble, tfidf_feats, avg_feats)\n",
        "    outputs = (logits,)\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    outputs = (loss,) + outputs\n",
        "    return outputs\n",
        "\n",
        "# Create instance of model and classificationHead\n",
        "hidden_size     = transformer_model.config.hidden_size\n",
        "num_labels      = config[\"num_labels\"]\n",
        "num_tfidf_feats = len(tfidf_vect.get_feature_names()) if config[\"ensemble\"] else 0\n",
        "dropout_prob    = config[\"dropout_prob\"]\n",
        "\n",
        "# Two different classification heads - distinct by a dropout layer\n",
        "classification_head = (BertClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob) if config[\"model_class\"] == \"BERT\"\n",
        "                      else RobertaClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob))\n",
        "\n",
        "model = Classifier(transformer_model, num_labels, classification_head)\n",
        "model = model.to(device)\n",
        "LEARNING_RATE    = config[\"learning_rate\"]\n",
        "EPOCHS           = config[\"epochs\"]\n",
        "WEIGHT_DECAY     = 0\n",
        "NUM_WARMUP_STEPS = 0\n",
        "TOTAL_STEPS      = len(train_dataloader) * EPOCHS\n",
        "if config[\"model_class\"] == \"RoBERTa\":\n",
        "  WEIGHT_DECAY     = 0.1\n",
        "  NUM_WARMUP_STEPS = int(0.06 * TOTAL_STEPS)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False, weight_decay=WEIGHT_DECAY) # BERT tf library use false.\n",
        "\n",
        "schedular = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() # returns max values on columns\n",
        "    labels_flat = labels.flatten()\n",
        "    assert len(pred_flat) == len(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "# define a dict with our dataloaders\n",
        "dataloader = {\n",
        "    'train' : train_dataloader,\n",
        "    'val'   : val_dataloader\n",
        "}\n",
        "\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "\n",
        "  for phase in ['train', 'val']:\n",
        "\n",
        "    if phase == 'train':\n",
        "      print('Running training step...\\n')\n",
        "\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc  = 0\n",
        "\n",
        "    if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "    else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "\n",
        "    for batch in dataloader[phase]:\n",
        "\n",
        "      #Unpack batch\n",
        "      input_ids      = batch.get('input_ids').to(device)\n",
        "      attention_mask = batch.get('attention_mask').to(device)\n",
        "      labels         = batch.get('label').to(device)\n",
        "      # Declare a none-type variable where model not ensemble - only concatenated where ensemble == True\n",
        "      tfidf_feats = None\n",
        "\n",
        "      if ENSEMBLE:\n",
        "        tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "\n",
        "        tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "\n",
        "      avg_feats      = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "\n",
        "      model.zero_grad()\n",
        "\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "        loss, logits = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              labels=labels,\n",
        "                              ensemble=ENSEMBLE,\n",
        "                              tfidf_feats=tfidf_feats,\n",
        "                              avg_feats=avg_feats) # Default is None, but need to declare a var for when ensemble True\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "\n",
        "        total_acc += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        if phase == 'train':\n",
        "\n",
        "\n",
        "          loss.backward()\n",
        "\n",
        "\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "\n",
        "          optimizer.step()\n",
        "\n",
        "\n",
        "          schedular.step()\n",
        "\n",
        "    avg_acc  = total_acc / len(dataloader[phase])\n",
        "    avg_loss = total_loss / len(dataloader[phase])\n",
        "\n",
        "    if phase == 'train':\n",
        "      print(\"Training step complete. \\n\")\n",
        "\n",
        "    print(\"   Average loss: {0:.2f}\".format(avg_loss))\n",
        "    print(\"   Average accuracy {0:.2f}\".format(avg_acc))\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "    history[phase +'_loss'].append(avg_loss)\n",
        "    history[phase +'_acc'].append(avg_acc)\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print('\\nPredicting the values.')\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_dataloader = test_dataloader if config['split_train'] else val_dataloader\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  #Unpack\n",
        "  input_ids      = batch.get('input_ids').to(device)\n",
        "  attention_mask = batch.get('attention_mask').to(device)\n",
        "  labels         = batch.get('label').to(device)\n",
        "  tfidf_feats    = None\n",
        "\n",
        "  if ENSEMBLE:\n",
        "    tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "\n",
        "    tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "\n",
        "  avg_feats = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      labels=labels,\n",
        "                      ensemble=ENSEMBLE,\n",
        "                      tfidf_feats=tfidf_feats,\n",
        "                      avg_feats=avg_feats)\n",
        "\n",
        "    logits = outputs[1]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('   DONE!')\n",
        "\n",
        "\n",
        "# Write to CSV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  y_preds.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_predictions = [item for sublist in y_preds for item in sublist]\n",
        "\n",
        "df = pd.DataFrame(flat_predictions,columns =['labels'])\n",
        "print(df)\n",
        "#print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "\n",
        "def compute_metrics(labels, predictions):\n",
        "\n",
        "    precision = precision_score(labels, predictions, pos_label=1, average='binary')\n",
        "    recall    = recall_score(labels, predictions, pos_label=1, average='binary')\n",
        "    # weighted f1-score\n",
        "    f1        = f1_score(labels, predictions, average='weighted')\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0,1]).ravel()\n",
        "\n",
        "    return {\n",
        "        \"precision\" : precision,\n",
        "        \"recall\"    : recall,\n",
        "        \"f1_score\"  : f1,\n",
        "        \"tp\"        : tp,\n",
        "        \"fp\"        : fp,\n",
        "        \"fn\"        : fn,\n",
        "        \"tn\"        : tn\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  with open('./transformer_model_tests.csv', 'a', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    metrics = compute_metrics(flat_true_labels, flat_predictions)\n",
        "\n",
        "    csv_writer.writerow([config['model_class'], config['model'], config['split_train'],\n",
        "                         config['preprocessed'], config['ensemble'],config['save_model'],\n",
        "                         config['random_seed'],config['max_len'], config['batch_size'],\n",
        "                         config['learning_rate'], config['epochs'], config['dropout_prob'],config['max_feats'],\n",
        "                         history['train_acc'], history['val_acc'], history['train_loss'],history['val_loss'],\n",
        "                         metrics['precision'], metrics['recall'], metrics['f1_score'],\n",
        "                         metrics['tp'], metrics['fp'], metrics['fn'], metrics['tn']])\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file trandormer_model_tests.csv' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "# Save Model\n",
        "if config['save_model']:\n",
        "\n",
        "  output_dir = './'+config['model']+'_save/'\n",
        "\n",
        "  # Create output directory if needed\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "  torch.save(model.state_dict(), output_dir + '_model_state.pt')\n",
        "  torch.save(model, output_dir + 'model.pt')\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Memory optimization\n",
        "del model, optimizer, schedular\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b7cc062952543ec8611eacd39cbf56b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a65aceccb14d496ab4c14e8511132a36",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5d31186deed4ab8b23b6a8af8a184e0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ea75a0f6c2149fabe685155a9343ecf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f44176b1b27345f99c4b7f4d83519092",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.70\n",
            "   Average accuracy 0.54\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.68\n",
            "   Average accuracy 0.87\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.61\n",
            "   Average accuracy 0.67\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.58\n",
            "   Average accuracy 0.73\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.34\n",
            "   Average accuracy 0.88\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.83\n",
            "   Average accuracy 0.74\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.14\n",
            "   Average accuracy 0.97\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.78\n",
            "   Average accuracy 0.78\n",
            "\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Predicting the values.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   DONE!\n",
            "     labels\n",
            "0         0\n",
            "1         1\n",
            "2         1\n",
            "3         0\n",
            "4         1\n",
            "..      ...\n",
            "995       1\n",
            "996       0\n",
            "997       0\n",
            "998       0\n",
            "999       0\n",
            "\n",
            "[1000 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ahh5-IEQaEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fyjVXfV3d0y"
      },
      "source": [
        "df1 = pd.read_csv(\"fakenewsoutput1.tsv\",sep=\"\\t\")\n",
        "sep=df1['Text']\n",
        "sep=sep.to_frame()\n",
        "type(sep)\n",
        "result = pd.concat([sep,df], axis=1)\n",
        "result.head()\n",
        "result.to_csv(\"Roberta.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jJqdpyYG2GU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b09f51448a4b46489093626eee3b829d",
            "493cc8a77b33484d8b1ce58f56fa0e40",
            "4c91ad4695bf4ad0b39ea727cd4234bd",
            "42c9423d9cdc4a63831113eb7beb0fb2",
            "a4cf3d667beb45cca2e79e12752809ee",
            "92c0ae7a32cf491baa6994ca0838195e",
            "bc5a9a7bd33d476d99695b4708cb5d9c",
            "c572724f886f4d5f973e901efacece11",
            "fdb1c71ed6b846d3aa397634b56a7300",
            "1e5840b5d1fd4810912466af9a86d6e7",
            "fbb516b92b744bf78ed7b4e5505ac7cd",
            "9cd4b06ad2e6419399bcfaed46dca250",
            "ce280a52c7de41dc84cab45ef4ff2bc0",
            "407cff14b3694c61bcaffb90394cb683",
            "80ba9119f80b4e32a366f037c3a97fcb",
            "cbeea8516f64426b9295f937a75cd3a2",
            "7765f5748f024cdd9da32ef581a21aa7",
            "9006be48038f49adaeff1006e07a1892",
            "cbb9df6b767b4cff8412ad3bcf3d67d0",
            "0676b8c819e34d79ac55dc5b332ad2b5",
            "dcbb5a05251148c889b6200e1e8bcaf8",
            "296598f76282494d8f19c266973298bf",
            "38136e3616e14b5fb8a5cd506f31dc13",
            "7368e19be953492a853aa062eb78c6a2",
            "0022b4489c5f457da646ca338c4b3177",
            "fa1411dcc5a447129bc30541cc543936",
            "fc88b6e6638449699ea6f1b8047a0855",
            "95d4eb1b0b6f481e818fddc29ce598ef",
            "98259c9325f44f67a6e89ac4abe9b193",
            "55ce51f2ce294a13bdb98a63f02f4a56",
            "733827eaaa3f47a8a03a7e140cd98c14",
            "aa6fd0e779344cb1a4922a59aa1f8124",
            "7175f082dea540f6832971ead2899e24",
            "e3e60f192fb44882ac170d37bbd0b891",
            "8bca92d880cd42708652d91a1dc58c28",
            "37732d263b1e43838c814ce7523b291e",
            "17626195152a40f0bec6b0751ee5f130",
            "a29b4243e43c4e4ca7665b1b1976220d",
            "45ee0f7b00fb4fe9a69acc575a0665dd",
            "58d0d0f8f84c4ba5a95b9ad79b2851e6",
            "f6932a37e0724c12972f21983a7ba748",
            "204507984b334521bae37a7d56c113ac",
            "9cecaa54ca2f4887992c58f592d5dd60",
            "476774eaaafd463ba77721560e532025",
            "b551a69c89e54ae7b0270f4a9ab132bb",
            "61f71bd185fc46718f6e791809b6f0f0",
            "e5dbfc13dfea4257a876d1c68ade3287",
            "dd1692edb5354bbc91a36ed70cff4996",
            "4813c01f9ea34ac6a85f046d4d59c606",
            "915a080687cb4c96865990d26a820aa0",
            "40e0dde29b6f4d8dbd55562ab4821f6d",
            "3b07f9a0ce334646b47f695ca0e8b684",
            "036c80b6d0e74b28a3e385d72ceb4fa8",
            "bd99a75d4f114c22a26f6f98aef190e2",
            "f1619d3792bf432a8c8c8ea7fac2a3cb"
          ]
        },
        "id": "lXZx7bM33iA4",
        "outputId": "87e8b156-ec21-4633-dffd-a29252caf0c5"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Functions\n",
        "\n",
        "def string_to_bool(string):\n",
        "\n",
        "    if string.lower() == 'false':\n",
        "        return False\n",
        "    elif string.lower() == 'true':\n",
        "        return True\n",
        "    else:\n",
        "        return ValueError\n",
        "\n",
        "\n",
        "def pre_process(tweet):\n",
        "\n",
        "    tweet = re.sub(\"HTTPURL\", \"http\", tweet)\n",
        "    tweet = \" \".join([\" \".join([word for word in re.sub('([A-Z][a-z]+)', r' \\1',\n",
        "                                                        re.sub('([A-Z]+)', r' \\1', word)).split()]) if word.startswith(\n",
        "        '#') else word for word in tweet.split()])\n",
        "    tweet = re.sub('# ', '#', tweet)\n",
        "\n",
        "    # convert instances of covid/corona to 'virus' for which there is an embedding\n",
        "    tweet = re.sub('(?i)CORONA$', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID-19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVIDー19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirusー19', 'coronavirus', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_emojis(data):\n",
        "\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "\n",
        "def handcrafted_feature(text):\n",
        "\n",
        "    text = remove_emojis(text)\n",
        "    text = [char for char in text if char not in string.punctuation]\n",
        "    count = len([char for char in text if char.isdigit()])\n",
        "    avg = (count/(len(text) - text.count(\" \")))*100\n",
        "    x =  torch.tensor([avg], dtype=torch.float)\n",
        "    y = x.view(1,1)\n",
        "    return y\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Set up arguments\n",
        "\n",
        "\n",
        "\n",
        "config     = {\"model_class\": \"BERT\",\n",
        "\"model\" : \"bert-base-uncased\",\n",
        "\"random_seed\": int(\"4\"),\n",
        "\"max_len\": int(\"128\"),\n",
        "\"epochs\": int(\"4\"),\n",
        "\"learning_rate\": float(\"1e-5\"),\n",
        "\"batch_size\": int(\"32\"),\n",
        "\"dropout_prob\": float(\"0.2\"),\n",
        "\"test_size\": float(\"0.3\"),\n",
        "\"preprocessed\": string_to_bool(\"True\"),\n",
        "\"ensemble\": string_to_bool(\"True\"),\n",
        "\"num_labels\": int(\"2\"),\n",
        "\"max_feats\": int(\"6000\"),\n",
        "\"save_model\": string_to_bool(\"False\"),\n",
        "\"split_train\": string_to_bool(\"False\")}\n",
        "\n",
        "\n",
        "\n",
        "train_data = None\n",
        "df_val     = None\n",
        "\n",
        "MODEL         = None\n",
        "RANDOM_SEED   = None\n",
        "MAX_LEN       = None\n",
        "BATCH_SIZE    = None\n",
        "EPOCHS        = None\n",
        "LEARNING_RATE = None\n",
        "TEST_SIZE     = None\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# read in data\n",
        "\n",
        "try:\n",
        "  train_data = pd.read_csv('train.tsv', sep=r'\\t', header=0)\n",
        "  #assert train_data.shape == (69, 3) , \"train.tsv should contain 7000 rows, only found \" + str(train_data.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/train.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "try:\n",
        "  df_val = pd.read_csv('fakenewsoutput.tsv', sep=r'\\t', header=0)\n",
        "  #assert df_val.shape == (9, 3) , \"valid.tsv should contain 1000 rows, only found \" + str(df_val.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/valid.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to train text and validation text\n",
        "if config['preprocessed']:\n",
        "  train_data.Text = train_data.Text.apply(lambda x: pre_process(x))\n",
        "  #print(df_val)\n",
        "  df_val.Text     = df_val.Text.apply(lambda x: pre_process(x))\n",
        "\n",
        "train_data.Label  = train_data.Label.apply(lambda x: 0 if x == 'UNINFORMATIVE' else 1)\n",
        "df_val.Label      = df_val.Label.apply(lambda x: 0 if x=='UNINFORMATIVE' else 1)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Configure Model\n",
        "\n",
        "TRANSFORMERS = {\n",
        "    \"BERT\" : (BertModel, BertTokenizer),\n",
        "    \"RoBERTa\": (RobertaModel, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "transformer_class, tokenizer_class = TRANSFORMERS.get(config[\"model_class\"])\n",
        "\n",
        "# Define model\n",
        "transformer_model = transformer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer   = tokenizer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define sequence length, batch size and random state\n",
        "MAX_LEN     = config['max_len']\n",
        "BATCH_SIZE  = config['batch_size']\n",
        "RANDOM_SEED = config['random_seed']\n",
        "TEST_SIZE   = config['test_size']\n",
        "ENSEMBLE    = config['ensemble']\n",
        "MAX_FEATS   = config['max_feats']\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "  # Propogate random_state\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "  torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "#transformer_model.config.model_type\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text, label, tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "        'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "if config['split_train']: # split training to create a test set\n",
        "  df_train, df_test = train_test_split(train_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "  test_dataloader   = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Create loaders for training, test and val data\n",
        "train_dataloader = (create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) if config['split_train']\n",
        "                    else create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE))\n",
        "val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# TF-IDF MODEL\n",
        "\n",
        "# Define a method that will passed as an analyzer to the TfidfVectorizer\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = \"\".join([char for char in text if not char.isdigit()])\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "if ENSEMBLE:\n",
        "\n",
        "  # Instantiate TfidfVectorizer object and pass clean_text method which it will automatically apply\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text, max_features=MAX_FEATS)\n",
        "\n",
        "  # Learning vocabulary on training data\n",
        "  if config['split_train']:\n",
        "        tfidf_vect.fit(df_train.Text)\n",
        "  else:\n",
        "        tfidf_vect.fit(train_data.Text)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Model Classes\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(RobertaClassificationHead, self).__init__()\n",
        "    self.input_size  = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels  = num_labels\n",
        "    self.dropout     = nn.Dropout(p=dropout_prob)\n",
        "    self.activation  = nn.Tanh()\n",
        "    #self.dense      = nn.Linear(hidden_size , hidden_size) # Note - atm hidden_size\n",
        "    self.dense      = nn.Linear(self.input_size , self.input_size) # Note - atm hidden_size\n",
        "    self.out_proj   = nn.Linear(self.input_size , self.num_labels)\n",
        "\n",
        "\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dropout(x) # Roberta has extra dropout layer, and experimentation showed this gets better results.\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BertClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(BertClassificationHead, self).__init__()\n",
        "    self.input_size = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels = num_labels\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dense      = nn.Linear(self.input_size, self.input_size)\n",
        "    self.dropout    = nn.Dropout(p=dropout_prob)\n",
        "    self.out_proj   = nn.Linear(self.input_size, self.num_labels)\n",
        "\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take [CLS]\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, model, num_labels, classification_head):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.num_labels = num_labels\n",
        "    self.classification_head = classification_head\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None, ensemble=None, tfidf_feats=None, avg_feats=None):\n",
        "\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "\n",
        "    model_output = outputs[0]\n",
        "\n",
        "    logits = self.classification_head(model_output, ensemble, tfidf_feats, avg_feats)\n",
        "\n",
        "    outputs = (logits,)\n",
        "\n",
        "\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    outputs = (loss,) + outputs\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Create instance of model and classificationHead\n",
        "hidden_size     = transformer_model.config.hidden_size\n",
        "num_labels      = config[\"num_labels\"]\n",
        "num_tfidf_feats = len(tfidf_vect.get_feature_names()) if config[\"ensemble\"] else 0\n",
        "dropout_prob    = config[\"dropout_prob\"]\n",
        "\n",
        "# Two different classification heads - distinct by a dropout layer\n",
        "classification_head = (BertClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob) if config[\"model_class\"] == \"BERT\"\n",
        "                      else RobertaClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob))\n",
        "\n",
        "model = Classifier(transformer_model, num_labels, classification_head)\n",
        "\n",
        "# Move to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "LEARNING_RATE    = config[\"learning_rate\"]\n",
        "EPOCHS           = config[\"epochs\"]\n",
        "\n",
        "WEIGHT_DECAY     = 0 # defaults to 0 in AdamW\n",
        "NUM_WARMUP_STEPS = 0\n",
        "TOTAL_STEPS      = len(train_dataloader) * EPOCHS\n",
        "\n",
        "if config[\"model_class\"] == \"RoBERTa\":\n",
        "  WEIGHT_DECAY     = 0.1\n",
        "  NUM_WARMUP_STEPS = int(0.06 * TOTAL_STEPS) # 0.06 ratio\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False, weight_decay=WEIGHT_DECAY) # BERT tf library use false.\n",
        "\n",
        "schedular = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() # returns max values on columns\n",
        "    labels_flat = labels.flatten()\n",
        "    assert len(pred_flat) == len(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Training Loop\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "# define a dict with our dataloaders\n",
        "dataloader = {\n",
        "    'train' : train_dataloader,\n",
        "    'val'   : val_dataloader\n",
        "}\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "\n",
        "  for phase in ['train', 'val']:\n",
        "\n",
        "    if phase == 'train':\n",
        "      print('Running training step...\\n')\n",
        "    else:\n",
        "      print('Running validation...\\n')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc  = 0\n",
        "\n",
        "    if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "    else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "\n",
        "    for batch in dataloader[phase]:\n",
        "\n",
        "      #Unpack batch\n",
        "      input_ids      = batch.get('input_ids').to(device)\n",
        "      attention_mask = batch.get('attention_mask').to(device)\n",
        "      labels         = batch.get('label').to(device)\n",
        "      # Declare a none-type variable where model not ensemble - only concatenated where ensemble == True\n",
        "      tfidf_feats = None\n",
        "\n",
        "      if ENSEMBLE:\n",
        "        tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "\n",
        "        tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "      # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "      avg_feats      = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "      # Pytorch accumulates the gradients on backward pass so need to set them to zero.\n",
        "      model.zero_grad()\n",
        "\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "        loss, logits = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              labels=labels,\n",
        "                              ensemble=ENSEMBLE,\n",
        "                              tfidf_feats=tfidf_feats,\n",
        "                              avg_feats=avg_feats) # Default is None, but need to declare a var for when ensemble True\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        total_acc += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        if phase == 'train':\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Use gradient clipping to prevent exploding gradients.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          schedular.step()\n",
        "\n",
        "    avg_acc  = total_acc / len(dataloader[phase])\n",
        "    avg_loss = total_loss / len(dataloader[phase])\n",
        "\n",
        "    if phase == 'train':\n",
        "      print(\"Training step complete. \\n\")\n",
        "   # else:\n",
        "     # print('Validation step complete. \\n')\n",
        "\n",
        "    print(\"   Average loss: {0:.2f}\".format(avg_loss))\n",
        "    print(\"   Average accuracy {0:.2f}\".format(avg_acc))\n",
        "    print(\"\")\n",
        "\n",
        "    # Record all statistics from this epoch and phase.\n",
        "    history[phase +'_loss'].append(avg_loss)\n",
        "    history[phase +'_acc'].append(avg_acc)\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print('\\nPrediction values')\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_dataloader = test_dataloader if config['split_train'] else val_dataloader\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  #Unpack\n",
        "  input_ids      = batch.get('input_ids').to(device)\n",
        "  attention_mask = batch.get('attention_mask').to(device)\n",
        "  labels         = batch.get('label').to(device)\n",
        "  tfidf_feats    = None\n",
        "\n",
        "  if ENSEMBLE:\n",
        "    tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "\n",
        "    tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "\n",
        "  avg_feats = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      labels=labels,\n",
        "                      ensemble=ENSEMBLE,\n",
        "                      tfidf_feats=tfidf_feats,\n",
        "                      avg_feats=avg_feats)\n",
        "\n",
        "    logits = outputs[1]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('   DONE!')\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Write to CSV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  y_preds.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_predictions = [item for sublist in y_preds for item in sublist]\n",
        "#print(flat_predictions)\n",
        "df11 = pd.DataFrame(flat_predictions,columns =['labels'])\n",
        "print(df11)\n",
        "#print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "\n",
        "def compute_metrics(labels, predictions):\n",
        "\n",
        "    # precision and recall only for INFORMATIVE class\n",
        "    precision = precision_score(labels, predictions, pos_label=1, average='binary')\n",
        "    recall    = recall_score(labels, predictions, pos_label=1, average='binary')\n",
        "    # weighted f1-score\n",
        "    f1        = f1_score(labels, predictions, average='weighted')\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0,1]).ravel()\n",
        "\n",
        "    return {\n",
        "        \"precision\" : precision,\n",
        "        \"recall\"    : recall,\n",
        "        \"f1_score\"  : f1,\n",
        "        \"tp\"        : tp,\n",
        "        \"fp\"        : fp,\n",
        "        \"fn\"        : fn,\n",
        "        \"tn\"        : tn\n",
        "    }\n",
        "\n",
        "#Write out to csv file:\n",
        "\n",
        "try:\n",
        "  with open('./transformer_model_tests.csv', 'a', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    metrics = compute_metrics(flat_true_labels, flat_predictions)\n",
        "\n",
        "    csv_writer.writerow([config['model_class'], config['model'], config['split_train'],\n",
        "                         config['preprocessed'], config['ensemble'],config['save_model'],\n",
        "                         config['random_seed'],config['max_len'], config['batch_size'],\n",
        "                         config['learning_rate'], config['epochs'], config['dropout_prob'],config['max_feats'],\n",
        "                         history['train_acc'], history['val_acc'], history['train_loss'],history['val_loss'],\n",
        "                         metrics['precision'], metrics['recall'], metrics['f1_score'],\n",
        "                         metrics['tp'], metrics['fp'], metrics['fn'], metrics['tn']])\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file trandormer_model_tests.csv' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Save Model\n",
        "if config['save_model']:\n",
        "\n",
        "  output_dir = './'+config['model']+'_save/'\n",
        "\n",
        "  # Create output directory if needed\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "\n",
        "  torch.save(model.state_dict(), output_dir + '_model_state.pt')\n",
        "  torch.save(model, output_dir + 'model.pt')\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Memory optimization\n",
        "del model, optimizer, schedular\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b09f51448a4b46489093626eee3b829d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cd4b06ad2e6419399bcfaed46dca250",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38136e3616e14b5fb8a5cd506f31dc13",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3e60f192fb44882ac170d37bbd0b891",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b551a69c89e54ae7b0270f4a9ab132bb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.69\n",
            "   Average accuracy 0.56\n",
            "\n",
            "Running validation...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.53\n",
            "   Average accuracy 0.81\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.31\n",
            "   Average accuracy 0.88\n",
            "\n",
            "Running validation...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.48\n",
            "   Average accuracy 0.82\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.10\n",
            "   Average accuracy 0.97\n",
            "\n",
            "Running validation...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.63\n",
            "   Average accuracy 0.82\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.04\n",
            "   Average accuracy 0.99\n",
            "\n",
            "Running validation...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.54\n",
            "   Average accuracy 0.84\n",
            "\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Prediction values\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   DONE!\n",
            "     labels\n",
            "0         0\n",
            "1         1\n",
            "2         1\n",
            "3         0\n",
            "4         1\n",
            "..      ...\n",
            "995       1\n",
            "996       0\n",
            "997       0\n",
            "998       0\n",
            "999       0\n",
            "\n",
            "[1000 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuVcpvmI3lA0"
      },
      "source": [
        "df1 = pd.read_csv(\"fakenewsoutput1.tsv\",sep=\"\\t\")\n",
        "sep=df1['Text']\n",
        "sep=sep.to_frame()\n",
        "type(sep)\n",
        "result = pd.concat([sep,df11], axis=1)\n",
        "result\n",
        "result.to_csv('bert.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d68d583e3e08432a8665418827557194",
            "a3ca5644a9014cf084aa4baa38f6b2f3",
            "44453920c8c74ee096e16f9be1e83637",
            "64e603c1577d4bb7a0e8f1df7aeedf64",
            "0c575190e3bd4d2db41e9fc0f46c282e",
            "e0f2b4d64b584515b9ee012afb631776",
            "aadc1f0bbc8048f28489c599c0cb741c",
            "3a7e3f6a87394b58990a46a18af6237a",
            "f9e4bb4f95294fc3a508a9db626f0708",
            "6ac801aac05948d2866f64e3f2640a6d",
            "b091d5c6016d4c2f8ce34fa1d6cdecdd",
            "00288cce28f84ed9bb00f11795478d55",
            "abcd4f8492cd4efcad3c9314a9738722",
            "cb24b20bee31485da8fa2afc30e561e2",
            "a18508a9157141fcbc867c718e9ded1b",
            "b4daa67006544f399c31b3feb1b3da3d",
            "f1b2b8ae3d584408a7bcef36b5b564cc",
            "430e741f529a4bf381591a66afb1d7fb",
            "e08cdc345f984d579fc0b28415999671",
            "b85dc7fceaf2437aadf5dc2091008939",
            "5f6fe31cc4f447cc8f51f12018ae2620",
            "995c5fceee2d4e02a2084ae699596ae8",
            "89fa44bdf1f44b629847e82433b18d2c",
            "5a89e0fc32724d009baa9b6da896bcf1",
            "3513d4ccf7724d549374dc4c8417496c",
            "b2e496797e504f04b5612586ff9d71c0",
            "406d4c7928254b0b985ffca0c0aa1de2",
            "1b3ffe85beb347efafc7a0c674ac3c9b",
            "6742a23fd305470687e96fdb4ebc75cc",
            "9588a858e5624a008b911ef2d33b798e",
            "a0b201a760844f76bd939ea3700327a6",
            "fe393216b80c4de6b5993ecd1ec26c79",
            "30c7b73987624a8ba0914a8f801edc25"
          ]
        },
        "id": "7m0_LRg03r8p",
        "outputId": "9f2269ba-6b28-49e4-9dfa-d4add83aea0a"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Functions\n",
        "\n",
        "def string_to_bool(string):\n",
        "\n",
        "    if string.lower() == 'false':\n",
        "        return False\n",
        "    elif string.lower() == 'true':\n",
        "        return True\n",
        "    else:\n",
        "        return ValueError\n",
        "\n",
        "\n",
        "def pre_process(tweet):\n",
        "\n",
        "    tweet = re.sub(\"HTTPURL\", \"http\", tweet)\n",
        "    tweet = \" \".join([\" \".join([word for word in re.sub('([A-Z][a-z]+)', r' \\1',\n",
        "                                                        re.sub('([A-Z]+)', r' \\1', word)).split()]) if word.startswith(\n",
        "        '#') else word for word in tweet.split()])\n",
        "    tweet = re.sub('# ', '#', tweet)\n",
        "\n",
        "    # convert instances of covid/corona to 'virus' for which there is an embedding\n",
        "    tweet = re.sub('(?i)CORONA$', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID-19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVIDー19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirusー19', 'coronavirus', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_emojis(data):\n",
        "\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "\n",
        "def handcrafted_feature(text):\n",
        "\n",
        "    text = remove_emojis(text)\n",
        "    text = [char for char in text if char not in string.punctuation]\n",
        "    count = len([char for char in text if char.isdigit()])\n",
        "    avg = (count/(len(text) - text.count(\" \")))*100\n",
        "    x =  torch.tensor([avg], dtype=torch.float)\n",
        "    y = x.view(1,1)\n",
        "    return y\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Set up arguments\n",
        "\n",
        "\n",
        "\n",
        "config     = {\"model_class\": \"CTBERT\",\n",
        "\"model\" : \"digitalepidemiologylab/covid-twitter-bert\",\n",
        "\"random_seed\": int(\"4\"),\n",
        "\"max_len\": int(\"128\"),\n",
        "\"epochs\": int(\"4\"),\n",
        "\"learning_rate\": float(\"1e-5\"),\n",
        "\"batch_size\": int(\"16\"),\n",
        "\"dropout_prob\": float(\"0.2\"),\n",
        "\"test_size\": float(\"0.3\"),\n",
        "\"preprocessed\": string_to_bool(\"True\"),\n",
        "\"ensemble\": string_to_bool(\"True\"),\n",
        "\"num_labels\": int(\"2\"),\n",
        "\"max_feats\": int(\"6000\"),\n",
        "\"save_model\": string_to_bool(\"False\"),\n",
        "\"split_train\": string_to_bool(\"False\")}\n",
        "\n",
        "\n",
        "\n",
        "train_data = None\n",
        "df_val     = None\n",
        "\n",
        "MODEL         = None\n",
        "RANDOM_SEED   = None\n",
        "MAX_LEN       = None\n",
        "BATCH_SIZE    = None\n",
        "EPOCHS        = None\n",
        "LEARNING_RATE = None\n",
        "TEST_SIZE     = None\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# read in data\n",
        "\n",
        "try:\n",
        "  train_data = pd.read_csv('train.tsv', sep=r'\\t', header=0)\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/train.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "try:\n",
        "  df_val = pd.read_csv('fakenewsoutput.tsv', sep=r'\\t', header=0)\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/valid.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to train text and validation text\n",
        "if config['preprocessed']:\n",
        "  train_data.Text = train_data.Text.apply(lambda x: pre_process(x))\n",
        "  #print(df_val)\n",
        "  df_val.Text     = df_val.Text.apply(lambda x: pre_process(x))\n",
        "\n",
        "train_data.Label  = train_data.Label.apply(lambda x: 0 if x == 'UNINFORMATIVE' else 1)\n",
        "df_val.Label      = df_val.Label.apply(lambda x: 0 if x=='UNINFORMATIVE' else 1)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Configure Model\n",
        "\n",
        "TRANSFORMERS = {\n",
        "    \"BERT\" : (BertModel, BertTokenizer)\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "transformer_class, tokenizer_class = TRANSFORMERS.get(config[\"model_class\"])\n",
        "\n",
        "# Define model\n",
        "transformer_model = transformer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer   = tokenizer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define sequence length, batch size and random state\n",
        "MAX_LEN     = config['max_len']\n",
        "BATCH_SIZE  = config['batch_size']\n",
        "RANDOM_SEED = config['random_seed']\n",
        "TEST_SIZE   = config['test_size']\n",
        "ENSEMBLE    = config['ensemble']\n",
        "MAX_FEATS   = config['max_feats']\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "  # Propogate random_state\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "  torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "#transformer_model.config.model_type\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Datasets and Dataloader\n",
        "\n",
        "\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text, label, tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "        'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=2\n",
        "  )\n",
        "\n",
        "if config['split_train']: # split training to create a test set\n",
        "  df_train, df_test = train_test_split(train_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "  test_dataloader   = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Create loaders for training, test and val data\n",
        "train_dataloader = (create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) if config['split_train']\n",
        "                    else create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE))\n",
        "val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# TF-IDF MODEL\n",
        "\n",
        "# Define a method that will passed as an analyzer to the TfidfVectorizer\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = \"\".join([char for char in text if not char.isdigit()])\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "if ENSEMBLE:\n",
        "\n",
        "  # Instantiate TfidfVectorizer object and pass clean_text method which it will automatically apply\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text, max_features=MAX_FEATS)\n",
        "\n",
        "  # Learning vocabulary on training data\n",
        "  if config['split_train']:\n",
        "        tfidf_vect.fit(df_train.Text)\n",
        "  else:\n",
        "        tfidf_vect.fit(train_data.Text)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Model Classes\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(RobertaClassificationHead, self).__init__()\n",
        "    self.input_size  = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels  = num_labels\n",
        "    self.dropout     = nn.Dropout(p=dropout_prob)\n",
        "    self.activation  = nn.Tanh()\n",
        "    #self.dense      = nn.Linear(hidden_size , hidden_size) # Note - atm hidden_size\n",
        "    self.dense      = nn.Linear(self.input_size , self.input_size) # Note - atm hidden_size\n",
        "    self.out_proj   = nn.Linear(self.input_size , self.num_labels)\n",
        "\n",
        "\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dropout(x) # Roberta has extra dropout layer, and experimentation showed this gets better results.\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class CtBertClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(BertClassificationHead, self).__init__()\n",
        "    self.input_size = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels = num_labels\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dense      = nn.Linear(self.input_size, self.input_size)\n",
        "    self.dropout    = nn.Dropout(p=dropout_prob)\n",
        "    self.out_proj   = nn.Linear(self.input_size, self.num_labels)\n",
        "\n",
        "\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take [CLS]\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, model, num_labels, classification_head):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.num_labels = num_labels\n",
        "    self.classification_head = classification_head\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None, ensemble=None, tfidf_feats=None, avg_feats=None):\n",
        "\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "\n",
        "    model_output = outputs[0]\n",
        "\n",
        "    logits = self.classification_head(model_output, ensemble, tfidf_feats, avg_feats)\n",
        "\n",
        "    outputs = (logits,)\n",
        "\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    outputs = (loss,) + outputs\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Create instance of model and classificationHead\n",
        "hidden_size     = transformer_model.config.hidden_size\n",
        "num_labels      = config[\"num_labels\"]\n",
        "num_tfidf_feats = len(tfidf_vect.get_feature_names()) if config[\"ensemble\"] else 0\n",
        "dropout_prob    = config[\"dropout_prob\"]\n",
        "\n",
        "\n",
        "classification_head = (CtBertClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob) if config[\"model_class\"] == \"BERT\"\n",
        "                      else RobertaClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob))\n",
        "\n",
        "model = Classifier(transformer_model, num_labels, classification_head)\n",
        "\n",
        "# Move to the GPU\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "LEARNING_RATE    = config[\"learning_rate\"]\n",
        "EPOCHS           = config[\"epochs\"]\n",
        "\n",
        "WEIGHT_DECAY     = 0 # defaults to 0 in AdamW\n",
        "NUM_WARMUP_STEPS = 0\n",
        "TOTAL_STEPS      = len(train_dataloader) * EPOCHS\n",
        "\n",
        "if config[\"model_class\"] == \"XLNetModel\":\n",
        "  WEIGHT_DECAY     = 0.1\n",
        "  NUM_WARMUP_STEPS = int(0.06 * TOTAL_STEPS) # 0.06 ratio\n",
        "\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False, weight_decay=WEIGHT_DECAY) # BERT tf library use false.\n",
        "\n",
        "\n",
        "schedular = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() # returns max values on columns\n",
        "    labels_flat = labels.flatten()\n",
        "    assert len(pred_flat) == len(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Training Loop\n",
        "\n",
        "history = defaultdict(list)\n",
        "\n",
        "# define a dict with our dataloaders\n",
        "dataloader = {\n",
        "    'train' : train_dataloader,\n",
        "    'val'   : val_dataloader\n",
        "}\n",
        "\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "\n",
        "  for phase in ['train', 'val']:\n",
        "\n",
        "    if phase == 'train':\n",
        "      print('Running training step...\\n')\n",
        "\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc  = 0\n",
        "\n",
        "    if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "    else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "\n",
        "    for batch in dataloader[phase]:\n",
        "\n",
        "      #Unpack batch\n",
        "      input_ids      = batch.get('input_ids').to(device)\n",
        "      attention_mask = batch.get('attention_mask').to(device)\n",
        "      labels         = batch.get('label').to(device)\n",
        "      # Declare a none-type variable where model not ensemble - only concatenated where ensemble == True\n",
        "      tfidf_feats = None\n",
        "\n",
        "      if ENSEMBLE:\n",
        "        tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "\n",
        "        tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "      # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "      avg_feats      = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "      # Pytorch accumulates the gradients on backward pass so need to set them to zero.\n",
        "      model.zero_grad()\n",
        "\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "        loss, logits = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              labels=labels,\n",
        "                              ensemble=ENSEMBLE,\n",
        "                              tfidf_feats=tfidf_feats,\n",
        "                              avg_feats=avg_feats) # Default is None, but need to declare a var for when ensemble True\n",
        "\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        total_acc += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        if phase == 'train':\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Use gradient clipping to prevent exploding gradients.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          schedular.step()\n",
        "\n",
        "    avg_acc  = total_acc / len(dataloader[phase])\n",
        "    avg_loss = total_loss / len(dataloader[phase])\n",
        "\n",
        "    if phase == 'train':\n",
        "      print(\"Training step complete. \\n\")\n",
        "\n",
        "\n",
        "    print(\"   Average loss: {0:.2f}\".format(avg_loss))\n",
        "    print(\"   Average accuracy {0:.2f}\".format(avg_acc))\n",
        "    print(\"\")\n",
        "\n",
        "    # Record all statistics from this epoch and phase.\n",
        "    history[phase +'_loss'].append(avg_loss)\n",
        "    history[phase +'_acc'].append(avg_acc)\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print('\\nPerforming Evaluation on Test Set.')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_dataloader = test_dataloader if config['split_train'] else val_dataloader\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  #Unpack\n",
        "  input_ids      = batch.get('input_ids').to(device)\n",
        "  attention_mask = batch.get('attention_mask').to(device)\n",
        "  labels         = batch.get('label').to(device)\n",
        "  tfidf_feats    = None\n",
        "\n",
        "  if ENSEMBLE:\n",
        "    tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "\n",
        "    tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "  avg_feats = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      labels=labels,\n",
        "                      ensemble=ENSEMBLE,\n",
        "                      tfidf_feats=tfidf_feats,\n",
        "                      avg_feats=avg_feats)\n",
        "\n",
        "    logits = outputs[1]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('   DONE!')\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Write to CSV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  y_preds.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_predictions = [item for sublist in y_preds for item in sublist]\n",
        "df22 = pd.DataFrame(flat_predictions,columns =['labels'])\n",
        "print(df22)\n",
        "#print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "\n",
        "def compute_metrics(labels, predictions):\n",
        "\n",
        "    # precision and recall only for INFORMATIVE class\n",
        "    precision = precision_score(labels, predictions, pos_label=1, average='binary')\n",
        "    recall    = recall_score(labels, predictions, pos_label=1, average='binary')\n",
        "    # weighted f1-score\n",
        "    f1        = f1_score(labels, predictions, average='weighted')\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0,1]).ravel()\n",
        "\n",
        "    return {\n",
        "        \"precision\" : precision,\n",
        "        \"recall\"    : recall,\n",
        "        \"f1_score\"  : f1,\n",
        "        \"tp\"        : tp,\n",
        "        \"fp\"        : fp,\n",
        "        \"fn\"        : fn,\n",
        "        \"tn\"        : tn\n",
        "    }\n",
        "\n",
        "#Write out to csv file:\n",
        "\n",
        "try:\n",
        "  with open('./transformer_model_tests.csv', 'a', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    metrics = compute_metrics(flat_true_labels, flat_predictions)\n",
        "\n",
        "    csv_writer.writerow([config['model_class'], config['model'], config['split_train'],\n",
        "                         config['preprocessed'], config['ensemble'],config['save_model'],\n",
        "                         config['random_seed'],config['max_len'], config['batch_size'],\n",
        "                         config['learning_rate'], config['epochs'], config['dropout_prob'],config['max_feats'],\n",
        "                         history['train_acc'], history['val_acc'], history['train_loss'],history['val_loss'],\n",
        "                         metrics['precision'], metrics['recall'], metrics['f1_score'],\n",
        "                         metrics['tp'], metrics['fp'], metrics['fn'], metrics['tn']])\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file trandormer_model_tests.csv' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Save Model\n",
        "if config['save_model']:\n",
        "\n",
        "  output_dir = './'+config['model']+'_save/'\n",
        "\n",
        "  # Create output directory if needed\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "  torch.save(model.state_dict(), output_dir + '_model_state.pt')\n",
        "  torch.save(model, output_dir + 'model.pt')\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Memory optimization\n",
        "del model, optimizer, schedular\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:109: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:116: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d68d583e3e08432a8665418827557194",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/421 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00288cce28f84ed9bb00f11795478d55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.25G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at digitalepidemiologylab/covid-twitter-bert were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89fa44bdf1f44b629847e82433b18d2c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.45\n",
            "   Average accuracy 0.79\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.80\n",
            "   Average accuracy 0.83\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.13\n",
            "   Average accuracy 0.97\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 0.99\n",
            "   Average accuracy 0.82\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.04\n",
            "   Average accuracy 0.99\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 1.22\n",
            "   Average accuracy 0.80\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Running training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.00\n",
            "   Average accuracy 1.00\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Average loss: 1.14\n",
            "   Average accuracy 0.82\n",
            "\n",
            "\n",
            "Training complete!\n",
            "\n",
            "Performing Evaluation on Test Set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   DONE!\n",
            "     labels\n",
            "0         0\n",
            "1         1\n",
            "2         1\n",
            "3         0\n",
            "4         0\n",
            "..      ...\n",
            "995       0\n",
            "996       0\n",
            "997       0\n",
            "998       0\n",
            "999       0\n",
            "\n",
            "[1000 rows x 1 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_wBpEJ_33dm"
      },
      "source": [
        "df1 = pd.read_csv(\"fakenewsoutput1.tsv\",sep=\"\\t\")\n",
        "sep=df1['Text']\n",
        "sep=sep.to_frame()\n",
        "type(sep)\n",
        "result = pd.concat([sep,df22], axis=1)\n",
        "result.to_csv('ctbert.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xmZOd-06dOd",
        "outputId": "898b50ad-7860-4e8d-c033-f1aca9cb8866"
      },
      "source": [
        "all=pd.concat([df,df11,df22], axis=1)\n",
        "all\n",
        "value=[]\n",
        "for i in range(0,len(all.index)):\n",
        "  s=all.iloc[i].tolist()\n",
        "  #print(s)\n",
        "  value.append(max(s,key=s.count))\n",
        "print(value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6FHCELK9pmG"
      },
      "source": [
        "final = pd.DataFrame(value,columns =['labels'])\n",
        "final\n",
        "result = pd.concat([sep,final], axis=1)\n",
        "result.to_csv('finaloutbert.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "wHl9LOstY-Ml",
        "outputId": "2e32e1a8-7be6-4636-ad03-a67408932466"
      },
      "source": [
        "result  = result.loc[(result[\"labels\"] ==1 )]\n",
        "display(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5928cc74-e2e9-46cc-be45-b826f9344dfc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Death happens all the time but is something we...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hoda Kotb Shares a Touching Tribute to a Belov...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The number of confirmed coronavirus cases in N...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@USER @USER Where is proof that thousand of pe...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>There are eight new cases of #COVID19 in #Sask...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>coronavirusupd indiafightscorona india recover...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>daili updat publish state report test case dea...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>third case arriv juli india stay isol stamford...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>total number confirm case covid number report ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>total number confirm case covid number report ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5928cc74-e2e9-46cc-be45-b826f9344dfc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5928cc74-e2e9-46cc-be45-b826f9344dfc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5928cc74-e2e9-46cc-be45-b826f9344dfc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  Text  labels\n",
              "1    Death happens all the time but is something we...       1\n",
              "2    Hoda Kotb Shares a Touching Tribute to a Belov...       1\n",
              "4    The number of confirmed coronavirus cases in N...       1\n",
              "6    @USER @USER Where is proof that thousand of pe...       1\n",
              "9    There are eight new cases of #COVID19 in #Sask...       1\n",
              "..                                                 ...     ...\n",
              "976  coronavirusupd indiafightscorona india recover...       1\n",
              "989  daili updat publish state report test case dea...       1\n",
              "991  third case arriv juli india stay isol stamford...       1\n",
              "993  total number confirm case covid number report ...       1\n",
              "995  total number confirm case covid number report ...       1\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "36mO0k62ZUMb",
        "outputId": "4ff8046c-4277-4bc7-bb97-b5630ab8938b"
      },
      "source": [
        "result.replace(1,\n",
        "           \"INFORMATIVE\",\n",
        "           inplace=True)\n",
        "\n",
        "result"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0d7a1b8a-613a-4217-93c5-076ddfd0e48c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Death happens all the time but is something we...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hoda Kotb Shares a Touching Tribute to a Belov...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The number of confirmed coronavirus cases in N...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@USER @USER Where is proof that thousand of pe...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>There are eight new cases of #COVID19 in #Sask...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>coronavirusupd indiafightscorona india recover...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>daili updat publish state report test case dea...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>third case arriv juli india stay isol stamford...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>total number confirm case covid number report ...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>total number confirm case covid number report ...</td>\n",
              "      <td>INFORMATIVE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d7a1b8a-613a-4217-93c5-076ddfd0e48c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0d7a1b8a-613a-4217-93c5-076ddfd0e48c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0d7a1b8a-613a-4217-93c5-076ddfd0e48c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  Text       labels\n",
              "1    Death happens all the time but is something we...  INFORMATIVE\n",
              "2    Hoda Kotb Shares a Touching Tribute to a Belov...  INFORMATIVE\n",
              "4    The number of confirmed coronavirus cases in N...  INFORMATIVE\n",
              "6    @USER @USER Where is proof that thousand of pe...  INFORMATIVE\n",
              "9    There are eight new cases of #COVID19 in #Sask...  INFORMATIVE\n",
              "..                                                 ...          ...\n",
              "976  coronavirusupd indiafightscorona india recover...  INFORMATIVE\n",
              "989  daili updat publish state report test case dea...  INFORMATIVE\n",
              "991  third case arriv juli india stay isol stamford...  INFORMATIVE\n",
              "993  total number confirm case covid number report ...  INFORMATIVE\n",
              "995  total number confirm case covid number report ...  INFORMATIVE\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwFRaZM8Z3Zg"
      },
      "source": [
        "result.to_csv('Bertmodelout.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIAK_HwIG6pb"
      },
      "source": [
        "## EXTRA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz481yCO3oYh",
        "outputId": "73caab44-e095-4593-a3e5-48ee5808103a"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 13.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 70.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlXpWcdhSyoe"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Functions\n",
        "\n",
        "def string_to_bool(string):\n",
        "    \"\"\"\n",
        "    Converts a string representation of a boolean to a boolean\n",
        "    :param string: A string \"True\" or \"False\"\n",
        "    :return: boolean value\n",
        "    \"\"\"\n",
        "    if string.lower() == 'false':\n",
        "        return False\n",
        "    elif string.lower() == 'true':\n",
        "        return True\n",
        "    else:\n",
        "        return ValueError\n",
        "\n",
        "\n",
        "def pre_process(tweet):\n",
        "    \"\"\"\n",
        "    Method for pre_processing tweet text:\n",
        "     * converting \"HTTPURL\" to \"http\" and splitting camelcased hashtags.\n",
        "     * parsing for \"covid-19\" and \"coronavirus\" and standardizing to \"coronavirus\"\n",
        "    Regex term for splitting derived from https://stackoverflow.com/questions/29916065/how-to-do-camelcase-split-in-python\n",
        "    with additional implementation for computation in list comprehension.\n",
        "    \"\"\"\n",
        "    tweet = re.sub(\"HTTPURL\", \"http\", tweet)\n",
        "    tweet = \" \".join([\" \".join([word for word in re.sub('([A-Z][a-z]+)', r' \\1',\n",
        "                                                        re.sub('([A-Z]+)', r' \\1', word)).split()]) if word.startswith(\n",
        "        '#') else word for word in tweet.split()])\n",
        "    tweet = re.sub('# ', '#', tweet)\n",
        "\n",
        "    # convert instances of covid/corona to 'virus' for which there is an embedding\n",
        "    tweet = re.sub('(?i)CORONA$', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID-19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVIDー19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirusー19', 'coronavirus', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_emojis(data):\n",
        "    \"\"\"\n",
        "    Function to remove unicode emojis.\n",
        "    :param data: The text data from which to remove emojis.\n",
        "    :return: The data with emojis removed.\n",
        "    \"\"\"\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "\n",
        "def handcrafted_feature(text):\n",
        "    \"\"\"\n",
        "    Function to return the percentage probability of any character in the text\n",
        "    being numeric. Parsing and removing emojis to prevent the influence of numbers\n",
        "    in unicode characters.\n",
        "    :param text: The tweet text.\n",
        "    :return: A (1,1) tensor of the probability of any character being numeric.\n",
        "    \"\"\"\n",
        "    text = remove_emojis(text)\n",
        "    text = [char for char in text if char not in string.punctuation]\n",
        "    count = len([char for char in text if char.isdigit()])\n",
        "    avg = (count/(len(text) - text.count(\" \")))*100\n",
        "    x =  torch.tensor([avg], dtype=torch.float)\n",
        "    y = x.view(1,1)\n",
        "    return y\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Set up arguments\n",
        "'''parser = argparse.ArgumentParser(description='Script that utalises task spooler to run multiple hyperparameter experiments')\n",
        "\n",
        "parser.add_argument('model_class', type=str, help='model class: BERT or RoBERTa')\n",
        "\n",
        "parser.add_argument('model', type=str, help='model: e.g.bert-base-uncased')\n",
        "\n",
        "parser.add_argument('random_seed', type=str)\n",
        "parser.add_argument('max_len', type=str)\n",
        "parser.add_argument('epochs', type=str)\n",
        "\n",
        "parser.add_argument('learning_rate', type=str)\n",
        "parser.add_argument('batch_size', type=str)\n",
        "parser.add_argument('dropout_prob', type=str)\n",
        "parser.add_argument('test_size', type=str)\n",
        "parser.add_argument('preprocessed', type=str)\n",
        "\n",
        "parser.add_argument('ensemble', type=str)\n",
        "parser.add_argument('num_labels', type=str)\n",
        "parser.add_argument('max_feats', type=str)\n",
        "parser.add_argument('save_model', type=str)\n",
        "parser.add_argument('split_train', type=str)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "config     = {\"model_class\": \"RoBERTa\",\n",
        "\"model\" : \"roberta-base\",\n",
        "\"random_seed\": int(\"4\"),\n",
        "\"max_len\": int(\"128\"),\n",
        "\"epochs\": int(\"4\"),\n",
        "\"learning_rate\": float(\"1e-5\"),\n",
        "\"batch_size\": int(\"32\"),\n",
        "\"dropout_prob\": float(\"0.2\"),\n",
        "\"test_size\": float(\"0.3\"),\n",
        "\"preprocessed\": string_to_bool(\"True\"),\n",
        "\"ensemble\": string_to_bool(\"True\"),\n",
        "\"num_labels\": int(\"2\"),\n",
        "\"max_feats\": int(\"6000\"),\n",
        "\"save_model\": string_to_bool(\"False\"),\n",
        "\"split_train\": string_to_bool(\"False\")}\n",
        "\n",
        "\n",
        "\n",
        "train_data = None\n",
        "df_val     = None\n",
        "\n",
        "MODEL         = None\n",
        "RANDOM_SEED   = None\n",
        "MAX_LEN       = None\n",
        "BATCH_SIZE    = None\n",
        "EPOCHS        = None\n",
        "LEARNING_RATE = None\n",
        "TEST_SIZE     = None\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# read in data\n",
        "\n",
        "try:\n",
        "  train_data = pd.read_csv('train.tsv', sep=r'\\t', header=0)\n",
        "  assert train_data.shape == (69, 3) , \"train.tsv should contain 7000 rows, only found \" + str(train_data.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/train.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "'''try:\n",
        "  df_val = pd.read_csv('valid.tsv', sep=r'\\t', header=0)\n",
        "  assert df_val.shape == (9, 3) , \"valid.tsv should contain 1000 rows, only found \" + str(df_val.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/valid.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit'''\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to train text and validation text\n",
        "if config['preprocessed']:\n",
        "  train_data.Text = train_data.Text.apply(lambda x: pre_process(x))\n",
        "  #print(df_val)\n",
        " # df_val.Text     = df_val.Text.apply(lambda x: pre_process(x))\n",
        "\n",
        "# Currently the labels in our dataframes are \"UNINFORMATIVE\" or \"INFORMATIVE\"\n",
        "# It is therefore neccessary to convert these to binary values for classification\n",
        "train_data.Label  = train_data.Label.apply(lambda x: 0 if x == 'UNINFORMATIVE' else 1)\n",
        "#df_val.Label      = df_val.Label.apply(lambda x: 0 if x=='UNINFORMATIVE' else 1)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Configure Model\n",
        "\n",
        "TRANSFORMERS = {\n",
        "    \"BERT\" : (BertModel, BertTokenizer),\n",
        "    \"RoBERTa\": (RobertaModel, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "transformer_class, tokenizer_class = TRANSFORMERS.get(config[\"model_class\"])\n",
        "\n",
        "# Define model\n",
        "transformer_model = transformer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer   = tokenizer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define sequence length, batch size and random state\n",
        "MAX_LEN     = config['max_len']\n",
        "BATCH_SIZE  = config['batch_size']\n",
        "RANDOM_SEED = config['random_seed']\n",
        "TEST_SIZE   = config['test_size']\n",
        "ENSEMBLE    = config['ensemble']\n",
        "MAX_FEATS   = config['max_feats']\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "  # Propogate random_state\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "  torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "#transformer_model.config.model_type\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Datasets and Dataloader\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Pytorch dataset classes are extensions of the base abstract dataset class\n",
        "# it is neccessary to override the __len__ method (returning the size of the dataset), and\n",
        "# __getitem__ (which defined how to get the next item in the dataset).\n",
        "#\n",
        "# For more info see (https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
        "#\n",
        "# This dataset returns a dictionary of the text, input_ids, attention_mask and label for a tweet.\n",
        "# It is important to return pytorch tensors, which is the format that the BERT model expects.\n",
        "#\n",
        "# Class derived from (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text, label, tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "        'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Wrap these datasets into pytorch dataloaders\n",
        "# Credit for this method goes to (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "if config['split_train']: # split training to create a test set\n",
        "  df_train, df_test = train_test_split(train_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        " # test_dataloader   = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Create loaders for training, test and val data\n",
        "train_dataloader = (create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) if config['split_train']\n",
        "                    else create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE))\n",
        "#val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# TF-IDF MODEL\n",
        "\n",
        "# Define a method that will passed as an analyzer to the TfidfVectorizer\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Method to clean the tweet text, removing case, removing stopwords and punctuation and tokenizing.\n",
        "    This will be parsed to the TfidfVectoriser as an analyzer and applied to each tweet in fitting.\n",
        "    :param text: The text of each tweet\n",
        "    :return: The text tokenized, with punctuation and stopwords removed.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = \"\".join([char for char in text if not char.isdigit()])\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "if ENSEMBLE:\n",
        "\n",
        "  # Instantiate TfidfVectorizer object and pass clean_text method which it will automatically apply\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text, max_features=MAX_FEATS)\n",
        "\n",
        "  # Learning vocabulary on training data\n",
        "  if config['split_train']:\n",
        "        tfidf_vect.fit(df_train.Text)\n",
        "  else:\n",
        "        tfidf_vect.fit(train_data.Text)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Model Classes\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(RobertaClassificationHead, self).__init__()\n",
        "    self.input_size  = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels  = num_labels\n",
        "    self.dropout     = nn.Dropout(p=dropout_prob)\n",
        "    self.activation  = nn.Tanh()\n",
        "    #self.dense      = nn.Linear(hidden_size , hidden_size) # Note - atm hidden_size\n",
        "    self.dense      = nn.Linear(self.input_size , self.input_size) # Note - atm hidden_size\n",
        "    self.out_proj   = nn.Linear(self.input_size , self.num_labels)\n",
        "\n",
        "  # Derived from (https://huggingface.co/transformers/v1.1.0/_modules/pytorch_transformers/modeling_roberta.html)\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dropout(x) # Roberta has extra dropout layer, and experimentation showed this gets better results.\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BertClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(BertClassificationHead, self).__init__()\n",
        "    self.input_size = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels = num_labels\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dense      = nn.Linear(self.input_size, self.input_size)\n",
        "    self.dropout    = nn.Dropout(p=dropout_prob)\n",
        "    self.out_proj   = nn.Linear(self.input_size, self.num_labels)\n",
        "\n",
        "  # Dervied from (https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertModel)\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take [CLS]\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, model, num_labels, classification_head):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.num_labels = num_labels\n",
        "    self.classification_head = classification_head\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None, ensemble=None, tfidf_feats=None, avg_feats=None):\n",
        "\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # if self.model.config.model_type == 'bert':\n",
        "    #   # 'outputs' is a tuple (sequence_output, pooled_output).\n",
        "    #   # The pooled_output is the last hidden-state of the [CLS] token further process by a Linear layer and tanh function\n",
        "    #   model_output = outputs[1]\n",
        "\n",
        "    # else:\n",
        "    #   # 'outputs' is raw hidden states from RoBERTa\n",
        "    #   # Therefore take the sequence output of dimensions (batch_size, max_len, hidden_size)\n",
        "    #   model_output = outputs[0]\n",
        "\n",
        "    # Need to avoid using pooled output from BERT (where making use of tfidf)\n",
        "    # therefore in each case BERT and RoBERTa going to use the sequence output\n",
        "    # and in the classification head extract the CLS token or equivalent\n",
        "\n",
        "    model_output = outputs[0]\n",
        "\n",
        "    logits = self.classification_head(model_output, ensemble, tfidf_feats, avg_feats)\n",
        "\n",
        "    outputs = (logits,)\n",
        "\n",
        "    # Following derived from (https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForSequenceClassification)\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    outputs = (loss,) + outputs\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Create instance of model and classificationHead\n",
        "hidden_size     = transformer_model.config.hidden_size\n",
        "num_labels      = config[\"num_labels\"]\n",
        "num_tfidf_feats = len(tfidf_vect.get_feature_names()) if config[\"ensemble\"] else 0\n",
        "dropout_prob    = config[\"dropout_prob\"]\n",
        "\n",
        "# Two different classification heads - distinct by a dropout layer\n",
        "classification_head = (BertClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob) if config[\"model_class\"] == \"BERT\"\n",
        "                      else RobertaClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob))\n",
        "\n",
        "model = Classifier(transformer_model, num_labels, classification_head)\n",
        "\n",
        "# Move to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "LEARNING_RATE    = config[\"learning_rate\"]\n",
        "EPOCHS           = config[\"epochs\"]\n",
        "\n",
        "WEIGHT_DECAY     = 0 # defaults to 0 in AdamW\n",
        "NUM_WARMUP_STEPS = 0\n",
        "TOTAL_STEPS      = len(train_dataloader) * EPOCHS\n",
        "\n",
        "if config[\"model_class\"] == \"RoBERTa\":\n",
        "  WEIGHT_DECAY     = 0.1\n",
        "  NUM_WARMUP_STEPS = int(0.06 * TOTAL_STEPS) # 0.06 ratio\n",
        "\n",
        "# Use a pytorch optimizer to preform updates on parameters.\n",
        "# This is the Adam algorithm with weight decay fix. This is the same optimizer\n",
        "# implemented in (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "# and the hugginface implementation of transformer models on the GLUE dataset,\n",
        "# see (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False, weight_decay=WEIGHT_DECAY) # BERT tf library use false.\n",
        "\n",
        "# Create a schedular which will update the learning rate.\n",
        "# This uses the defualt num_warmup_steps from:\n",
        "# (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L392)\n",
        "schedular = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "# Function to calculate the accuracy of a batch of predictions vs true labels\n",
        "# Function is derived from (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() # returns max values on columns\n",
        "    labels_flat = labels.flatten()\n",
        "    assert len(pred_flat) == len(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Training Loop\n",
        "\n",
        "# The below code forms the fine-tuning training loop of our model on the new data.\n",
        "# This code was influenced by (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk).\n",
        "# With ammendments to reflect the change that our dataset returns a dictionary, and the introduction of a phase for loop to reduce visual complexity.\n",
        "\n",
        "# As in (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "# we will store the losses associated with training and validiation.\n",
        "history = defaultdict(list)\n",
        "\n",
        "# define a dict with our dataloaders\n",
        "dataloader = {\n",
        "    'train' : train_dataloader,\n",
        " #   'val'   : val_dataloader\n",
        "}\n",
        "\n",
        "# Phase concept derived from (https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code)\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "\n",
        "  for phase in ['train']:\n",
        "\n",
        "    if phase == 'train':\n",
        "      print('Running training step...\\n')\n",
        "    else:\n",
        "      print('Running validation...\\n')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc  = 0\n",
        "\n",
        "    if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "    else:\n",
        "        model.eval\n",
        "        ()   # Set model to evaluate mode\n",
        "\n",
        "\n",
        "    for batch in dataloader[phase]:\n",
        "\n",
        "      #Unpack batch\n",
        "      input_ids      = batch.get('input_ids').to(device)\n",
        "      attention_mask = batch.get('attention_mask').to(device)\n",
        "      labels         = batch.get('label').to(device)\n",
        "      # Declare a none-type variable where model not ensemble - only concatenated where ensemble == True\n",
        "      tfidf_feats = None\n",
        "\n",
        "      if ENSEMBLE:\n",
        "        tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "        #transform to tensor - derived from https://github.com/donglinchen/text_classification/blob/master/model_pytorch.ipynb\n",
        "        tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "      # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "      avg_feats      = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "      # Pytorch accumulates the gradients on backward pass so need to set them to zero.\n",
        "      model.zero_grad()\n",
        "\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "        # Perform a forward pass on this training batch.\n",
        "        # As explained in (https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification),\n",
        "        # because we pass the true labels, the classification loss is returned along with the classification\n",
        "        # scores before normalizaition (\"logits\")\n",
        "        loss, logits = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              labels=labels,\n",
        "                              ensemble=ENSEMBLE,\n",
        "                              tfidf_feats=tfidf_feats,\n",
        "                              avg_feats=avg_feats) # Default is None, but need to declare a var for when ensemble True\n",
        "\n",
        "        # Accumulate the training loss to calculate average loss for this epoch.\n",
        "        # loss.item() extracts the value from the loss tensor as a python float.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        total_acc += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        if phase == 'train':\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Use gradient clipping to prevent exploding gradients.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          schedular.step()\n",
        "\n",
        "    avg_acc  = total_acc / len(dataloader[phase])\n",
        "    avg_loss = total_loss / len(dataloader[phase])\n",
        "\n",
        "    if phase == 'train':\n",
        "      print(\"Training step complete. \\n\")\n",
        "    else:\n",
        "      print('Validation step complete. \\n')\n",
        "\n",
        "    print(\"   Average loss: {0:.2f}\".format(avg_loss))\n",
        "    print(\"   Average accuracy {0:.2f}\".format(avg_acc))\n",
        "    print(\"\")\n",
        "\n",
        "    # Record all statistics from this epoch and phase.\n",
        "    history[phase +'_loss'].append(avg_loss)\n",
        "    history[phase +'_acc'].append(avg_acc)\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "print('\\nPerforming Evaluation on Test Set.')\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_dataloader = test_dataloader if config['split_train'] else val_dataloader\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  #Unpack\n",
        "  input_ids      = batch.get('input_ids').to(device)\n",
        "  attention_mask = batch.get('attention_mask').to(device)\n",
        "  labels         = batch.get('label').to(device)\n",
        "  tfidf_feats    = None\n",
        "\n",
        "  if ENSEMBLE:\n",
        "    tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "    #transform to tensor - derived from https://github.com/donglinchen/text_classification/blob/master/model_pytorch.ipynb\n",
        "    tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "  # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "  avg_feats = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      labels=labels,\n",
        "                      ensemble=ENSEMBLE,\n",
        "                      tfidf_feats=tfidf_feats,\n",
        "                      avg_feats=avg_feats)\n",
        "\n",
        "    logits = outputs[1]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('   DONE!')\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Write to CSV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# The predictions for each batch are a 2-column ndarray (one column for \"0\"\n",
        "# and one column for \"1\"). With rows refelcting the weight\n",
        "# of the predicition for each label. Identify the highest value and covert\n",
        "# in to a list of 0s and 1s.\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  y_preds.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "# Y_preds stores an array for each batch in our predicitions\n",
        "# to produce a classification report, extract each item in these sublists to a\n",
        "# flattend array.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_predictions = [item for sublist in y_preds for item in sublist]\n",
        "\n",
        "df = pd.DataFrame(flat_predictions,columns =['labels'])\n",
        "print(df)\n",
        "print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "\n",
        "def compute_metrics(labels, predictions):\n",
        "\n",
        "    # precision and recall only for INFORMATIVE class\n",
        "    precision = precision_score(labels, predictions, pos_label=1, average='binary')\n",
        "    recall    = recall_score(labels, predictions, pos_label=1, average='binary')\n",
        "    # weighted f1-score\n",
        "    f1        = f1_score(labels, predictions, average='weighted')\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0,1]).ravel()\n",
        "\n",
        "    return {\n",
        "        \"precision\" : precision,\n",
        "        \"recall\"    : recall,\n",
        "        \"f1_score\"  : f1,\n",
        "        \"tp\"        : tp,\n",
        "        \"fp\"        : fp,\n",
        "        \"fn\"        : fn,\n",
        "        \"tn\"        : tn\n",
        "    }\n",
        "\n",
        "#Write out to csv file:\n",
        "\n",
        "try:\n",
        "  with open('./transformer_model_tests.csv', 'a', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    metrics = compute_metrics(flat_true_labels, flat_predictions)\n",
        "\n",
        "    csv_writer.writerow([config['model_class'], config['model'], config['split_train'],\n",
        "                         config['preprocessed'], config['ensemble'],config['save_model'],\n",
        "                         config['random_seed'],config['max_len'], config['batch_size'],\n",
        "                         config['learning_rate'], config['epochs'], config['dropout_prob'],config['max_feats'],\n",
        "                         history['train_acc'], history['val_acc'], history['train_loss'],history['val_loss'],\n",
        "                         metrics['precision'], metrics['recall'], metrics['f1_score'],\n",
        "                         metrics['tp'], metrics['fp'], metrics['fn'], metrics['tn']])\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file trandormer_model_tests.csv' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Save Model\n",
        "if config['save_model']:\n",
        "\n",
        "  output_dir = './'+config['model']+'_save/'\n",
        "\n",
        "  # Create output directory if needed\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "  # Pytorch reccomends saving models using save/load state dict\n",
        "  # see (https://pytorch.org/tutorials/beginner/saving_loading_models.html#:~:text=Save%2FLoad%20state_dict%20(Recommended)&text=Saving%20the%20model's%20state_dict%20with,pt%20or%20.)\n",
        "  torch.save(model.state_dict(), output_dir + '_model_state.pt')\n",
        "  torch.save(model, output_dir + 'model.pt')\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Memory optimization\n",
        "del model, optimizer, schedular\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "C3qxe6DTS5T-",
        "outputId": "baef77e6-90d1-4519-b5bf-d37ea4a333a4"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Functions\n",
        "\n",
        "def string_to_bool(string):\n",
        "    \"\"\"\n",
        "    Converts a string representation of a boolean to a boolean\n",
        "    :param string: A string \"True\" or \"False\"\n",
        "    :return: boolean value\n",
        "    \"\"\"\n",
        "    if string.lower() == 'false':\n",
        "        return False\n",
        "    elif string.lower() == 'true':\n",
        "        return True\n",
        "    else:\n",
        "        return ValueError\n",
        "\n",
        "\n",
        "def pre_process(tweet):\n",
        "    \"\"\"\n",
        "    Method for pre_processing tweet text:\n",
        "     * converting \"HTTPURL\" to \"http\" and splitting camelcased hashtags.\n",
        "     * parsing for \"covid-19\" and \"coronavirus\" and standardizing to \"coronavirus\"\n",
        "    Regex term for splitting derived from https://stackoverflow.com/questions/29916065/how-to-do-camelcase-split-in-python\n",
        "    with additional implementation for computation in list comprehension.\n",
        "    \"\"\"\n",
        "    tweet = re.sub(\"HTTPURL\", \"http\", tweet)\n",
        "    tweet = \" \".join([\" \".join([word for word in re.sub('([A-Z][a-z]+)', r' \\1',\n",
        "                                                        re.sub('([A-Z]+)', r' \\1', word)).split()]) if word.startswith(\n",
        "        '#') else word for word in tweet.split()])\n",
        "    tweet = re.sub('# ', '#', tweet)\n",
        "\n",
        "    # convert instances of covid/corona to 'virus' for which there is an embedding\n",
        "    tweet = re.sub('(?i)CORONA$', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID-19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVIDー19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirusー19', 'coronavirus', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_emojis(data):\n",
        "    \"\"\"\n",
        "    Function to remove unicode emojis.\n",
        "    :param data: The text data from which to remove emojis.\n",
        "    :return: The data with emojis removed.\n",
        "    \"\"\"\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "\n",
        "def handcrafted_feature(text):\n",
        "    \"\"\"\n",
        "    Function to return the percentage probability of any character in the text\n",
        "    being numeric. Parsing and removing emojis to prevent the influence of numbers\n",
        "    in unicode characters.\n",
        "    :param text: The tweet text.\n",
        "    :return: A (1,1) tensor of the probability of any character being numeric.\n",
        "    \"\"\"\n",
        "    text = remove_emojis(text)\n",
        "    text = [char for char in text if char not in string.punctuation]\n",
        "    count = len([char for char in text if char.isdigit()])\n",
        "    avg = (count/(len(text) - text.count(\" \")))*100\n",
        "    x =  torch.tensor([avg], dtype=torch.float)\n",
        "    y = x.view(1,1)\n",
        "    return y\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Set up arguments\n",
        "'''parser = argparse.ArgumentParser(description='Script that utalises task spooler to run multiple hyperparameter experiments')\n",
        "\n",
        "parser.add_argument('model_class', type=str, help='model class: BERT or RoBERTa')\n",
        "\n",
        "parser.add_argument('model', type=str, help='model: e.g.bert-base-uncased')\n",
        "\n",
        "parser.add_argument('random_seed', type=str)\n",
        "parser.add_argument('max_len', type=str)\n",
        "parser.add_argument('epochs', type=str)\n",
        "\n",
        "parser.add_argument('learning_rate', type=str)\n",
        "parser.add_argument('batch_size', type=str)\n",
        "parser.add_argument('dropout_prob', type=str)\n",
        "parser.add_argument('test_size', type=str)\n",
        "parser.add_argument('preprocessed', type=str)\n",
        "\n",
        "parser.add_argument('ensemble', type=str)\n",
        "parser.add_argument('num_labels', type=str)\n",
        "parser.add_argument('max_feats', type=str)\n",
        "parser.add_argument('save_model', type=str)\n",
        "parser.add_argument('split_train', type=str)\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "'''\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"parser = argparse.ArgumentParser(description='Script that utalises task spooler to run multiple hyperparameter experiments')\\n\\nparser.add_argument('model_class', type=str, help='model class: BERT or RoBERTa')\\n\\nparser.add_argument('model', type=str, help='model: e.g.bert-base-uncased')\\n\\nparser.add_argument('random_seed', type=str)\\nparser.add_argument('max_len', type=str)\\nparser.add_argument('epochs', type=str)\\n\\nparser.add_argument('learning_rate', type=str)\\nparser.add_argument('batch_size', type=str)\\nparser.add_argument('dropout_prob', type=str)\\nparser.add_argument('test_size', type=str)\\nparser.add_argument('preprocessed', type=str)\\n\\nparser.add_argument('ensemble', type=str)\\nparser.add_argument('num_labels', type=str)\\nparser.add_argument('max_feats', type=str)\\nparser.add_argument('save_model', type=str)\\nparser.add_argument('split_train', type=str)\\n\\nargs = parser.parse_args()\\n\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOgNqk3BTCGf",
        "outputId": "60f31db9-08fa-469c-94a6-bd35860a2160"
      },
      "source": [
        "config     = {\"model_class\": \"RoBERTa\",\n",
        "\"model\" : \"roberta-base\",\n",
        "\"random_seed\": int(\"4\"),\n",
        "\"max_len\": int(\"128\"),\n",
        "\"epochs\": int(\"4\"),\n",
        "\"learning_rate\": float(\"1e-5\"),\n",
        "\"batch_size\": int(\"32\"),\n",
        "\"dropout_prob\": float(\"0.2\"),\n",
        "\"test_size\": float(\"0.3\"),\n",
        "\"preprocessed\": string_to_bool(\"True\"),\n",
        "\"ensemble\": string_to_bool(\"True\"),\n",
        "\"num_labels\": int(\"2\"),\n",
        "\"max_feats\": int(\"6000\"),\n",
        "\"save_model\": string_to_bool(\"False\"),\n",
        "\"split_train\": string_to_bool(\"False\")}\n",
        "\n",
        "\n",
        "\n",
        "train_data = None\n",
        "df_val     = None\n",
        "\n",
        "MODEL         = None\n",
        "RANDOM_SEED   = None\n",
        "MAX_LEN       = None\n",
        "BATCH_SIZE    = None\n",
        "EPOCHS        = None\n",
        "LEARNING_RATE = None\n",
        "TEST_SIZE     = None\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# read in data\n",
        "\n",
        "try:\n",
        "  train_data = pd.read_csv('train.tsv', sep=r'\\t', header=0)\n",
        "  assert train_data.shape == (69, 3) , \"train.tsv should contain 7000 rows, only found \" + str(train_data.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/train.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "try:\n",
        "  df_val = pd.read_csv('valid.tsv', sep=r'\\t', header=0)\n",
        "  #assert df_val.shape == (9, 3) , \"valid.tsv should contain 1000 rows, only found \" + str(df_val.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/valid.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhtLnipkTKpS"
      },
      "source": [
        "\n",
        "\n",
        "# Apply preprocessing to train text and validation text\n",
        "if config['preprocessed']:\n",
        "  train_data.Text = train_data.Text.apply(lambda x: pre_process(x))\n",
        "  #print(df_val)\n",
        "  df_val.Text     = df_val.Text.apply(lambda x: pre_process(x))\n",
        "\n",
        "# Currently the labels in our dataframes are \"UNINFORMATIVE\" or \"INFORMATIVE\"\n",
        "# It is therefore neccessary to convert these to binary values for classification\n",
        "train_data.Label  = train_data.Label.apply(lambda x: 0 if x == 'UNINFORMATIVE' else 1)\n",
        "#df_val.Label      = df_val.Label.apply(lambda x: 0 if x=='UNINFORMATIVE' else 1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cn_RBVjrTSg9",
        "outputId": "19c741d8-4440-49a0-ecec-85cea56faa8a"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Configure Model\n",
        "\n",
        "TRANSFORMERS = {\n",
        "    \"BERT\" : (BertModel, BertTokenizer),\n",
        "    \"RoBERTa\": (RobertaModel, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "transformer_class, tokenizer_class = TRANSFORMERS.get(config[\"model_class\"])\n",
        "\n",
        "# Define model\n",
        "transformer_model = transformer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer   = tokenizer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define sequence length, batch size and random state\n",
        "MAX_LEN     = config['max_len']\n",
        "BATCH_SIZE  = config['batch_size']\n",
        "RANDOM_SEED = config['random_seed']\n",
        "TEST_SIZE   = config['test_size']\n",
        "ENSEMBLE    = config['ensemble']\n",
        "MAX_FEATS   = config['max_feats']\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "  # Propogate random_state\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "  torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "#transformer_model.config.model_type\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIyq4i2AT7mg",
        "outputId": "b9e1b1e1-aa8a-4ed4-e776-0c16682c8034"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Datasets and Dataloader\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Pytorch dataset classes are extensions of the base abstract dataset class\n",
        "# it is neccessary to override the __len__ method (returning the size of the dataset), and\n",
        "# __getitem__ (which defined how to get the next item in the dataset).\n",
        "#\n",
        "# For more info see (https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
        "#\n",
        "# This dataset returns a dictionary of the text, input_ids, attention_mask and label for a tweet.\n",
        "# It is important to return pytorch tensors, which is the format that the BERT model expects.\n",
        "#\n",
        "# Class derived from (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text, label, tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "        'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Wrap these datasets into pytorch dataloaders\n",
        "# Credit for this method goes to (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "if config['split_train']: # split training to create a test set\n",
        "  df_train, df_test = train_test_split(train_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "  test_dataloader   = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Create loaders for training, test and val data\n",
        "train_dataloader = (create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) if config['split_train']\n",
        "                    else create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE))\n",
        "#val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0xwCth_UJut",
        "outputId": "ddebd11e-72db-4f77-8354-50297de49a30"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Datasets and Dataloader\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Pytorch dataset classes are extensions of the base abstract dataset class\n",
        "# it is neccessary to override the __len__ method (returning the size of the dataset), and\n",
        "# __getitem__ (which defined how to get the next item in the dataset).\n",
        "#\n",
        "# For more info see (https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
        "#\n",
        "# This dataset returns a dictionary of the text, input_ids, attention_mask and label for a tweet.\n",
        "# It is important to return pytorch tensors, which is the format that the BERT model expects.\n",
        "#\n",
        "# Class derived from (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text,  tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    #self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "     #   'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Wrap these datasets into pytorch dataloaders\n",
        "# Credit for this method goes to (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      #label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb455K2RUJ5U"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# TF-IDF MODEL\n",
        "\n",
        "# Define a method that will passed as an analyzer to the TfidfVectorizer\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Method to clean the tweet text, removing case, removing stopwords and punctuation and tokenizing.\n",
        "    This will be parsed to the TfidfVectoriser as an analyzer and applied to each tweet in fitting.\n",
        "    :param text: The text of each tweet\n",
        "    :return: The text tokenized, with punctuation and stopwords removed.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = \"\".join([char for char in text if not char.isdigit()])\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "if ENSEMBLE:\n",
        "\n",
        "  # Instantiate TfidfVectorizer object and pass clean_text method which it will automatically apply\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text, max_features=MAX_FEATS)\n",
        "\n",
        "  # Learning vocabulary on training data\n",
        "  if config['split_train']:\n",
        "        tfidf_vect.fit(df_train.Text)\n",
        "  else:\n",
        "        tfidf_vect.fit(train_data.Text)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqZgwu-dUgZX"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Model Classes\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(RobertaClassificationHead, self).__init__()\n",
        "    self.input_size  = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels  = num_labels\n",
        "    self.dropout     = nn.Dropout(p=dropout_prob)\n",
        "    self.activation  = nn.Tanh()\n",
        "    #self.dense      = nn.Linear(hidden_size , hidden_size) # Note - atm hidden_size\n",
        "    self.dense      = nn.Linear(self.input_size , self.input_size) # Note - atm hidden_size\n",
        "    self.out_proj   = nn.Linear(self.input_size , self.num_labels)\n",
        "\n",
        "  # Derived from (https://huggingface.co/transformers/v1.1.0/_modules/pytorch_transformers/modeling_roberta.html)\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dropout(x) # Roberta has extra dropout layer, and experimentation showed this gets better results.\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BertClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(BertClassificationHead, self).__init__()\n",
        "    self.input_size = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels = num_labels\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dense      = nn.Linear(self.input_size, self.input_size)\n",
        "    self.dropout    = nn.Dropout(p=dropout_prob)\n",
        "    self.out_proj   = nn.Linear(self.input_size, self.num_labels)\n",
        "\n",
        "  # Dervied from (https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertModel)\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take [CLS]\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, model, num_labels, classification_head):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.num_labels = num_labels\n",
        "    self.classification_head = classification_head\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None, ensemble=None, tfidf_feats=None, avg_feats=None):\n",
        "\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # if self.model.config.model_type == 'bert':\n",
        "    #   # 'outputs' is a tuple (sequence_output, pooled_output).\n",
        "    #   # The pooled_output is the last hidden-state of the [CLS] token further process by a Linear layer and tanh function\n",
        "    #   model_output = outputs[1]\n",
        "\n",
        "    # else:\n",
        "    #   # 'outputs' is raw hidden states from RoBERTa\n",
        "    #   # Therefore take the sequence output of dimensions (batch_size, max_len, hidden_size)\n",
        "    #   model_output = outputs[0]\n",
        "\n",
        "    # Need to avoid using pooled output from BERT (where making use of tfidf)\n",
        "    # therefore in each case BERT and RoBERTa going to use the sequence output\n",
        "    # and in the classification head extract the CLS token or equivalent\n",
        "\n",
        "    model_output = outputs[0]\n",
        "\n",
        "    logits = self.classification_head(model_output, ensemble, tfidf_feats, avg_feats)\n",
        "\n",
        "    outputs = (logits,)\n",
        "\n",
        "    # Following derived from (https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForSequenceClassification)\n",
        "   # loss_fct = nn.CrossEntropyLoss()\n",
        "    #loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    #outputs = (loss,) + outputs\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLc9oiZEUoTI",
        "outputId": "3d6d9558-0a30-4839-f4e6-30a2334e3f5f"
      },
      "source": [
        "# ==============================================================================================================================================\n",
        "# Create instance of model and classificationHead\n",
        "hidden_size     = transformer_model.config.hidden_size\n",
        "num_labels      = config[\"num_labels\"]\n",
        "num_tfidf_feats = len(tfidf_vect.get_feature_names()) if config[\"ensemble\"] else 0\n",
        "dropout_prob    = config[\"dropout_prob\"]\n",
        "\n",
        "# Two different classification heads - distinct by a dropout layer\n",
        "classification_head = (BertClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob) if config[\"model_class\"] == \"BERT\"\n",
        "                      else RobertaClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob))\n",
        "\n",
        "model = Classifier(transformer_model, num_labels, classification_head)\n",
        "\n",
        "# Move to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "LEARNING_RATE    = config[\"learning_rate\"]\n",
        "EPOCHS           = config[\"epochs\"]\n",
        "\n",
        "WEIGHT_DECAY     = 0 # defaults to 0 in AdamW\n",
        "NUM_WARMUP_STEPS = 0\n",
        "TOTAL_STEPS      = len(train_dataloader) * EPOCHS\n",
        "\n",
        "if config[\"model_class\"] == \"RoBERTa\":\n",
        "  WEIGHT_DECAY     = 0.1\n",
        "  NUM_WARMUP_STEPS = int(0.06 * TOTAL_STEPS) # 0.06 ratio\n",
        "\n",
        "# Use a pytorch optimizer to preform updates on parameters.\n",
        "# This is the Adam algorithm with weight decay fix. This is the same optimizer\n",
        "# implemented in (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "# and the hugginface implementation of transformer models on the GLUE dataset,\n",
        "# see (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False, weight_decay=WEIGHT_DECAY) # BERT tf library use false.\n",
        "\n",
        "# Create a schedular which will update the learning rate.\n",
        "# This uses the defualt num_warmup_steps from:\n",
        "# (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L392)\n",
        "schedular = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "# Function to calculate the accuracy of a batch of predictions vs true labels\n",
        "# Function is derived from (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() # returns max values on columns\n",
        "    labels_flat = labels.flatten()\n",
        "    assert len(pred_flat) == len(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Training Loop\n",
        "\n",
        "# The below code forms the fine-tuning training loop of our model on the new data.\n",
        "# This code was influenced by (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk).\n",
        "# With ammendments to reflect the change that our dataset returns a dictionary, and the introduction of a phase for loop to reduce visual complexity.\n",
        "\n",
        "# As in (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "# we will store the losses associated with training and validiation.\n",
        "history = defaultdict(list)\n",
        "\n",
        "# define a dict with our dataloaders\n",
        "dataloader = {\n",
        "    #'train' : train_dataloader,\n",
        "    'val'   : val_dataloader\n",
        "}\n",
        "\n",
        "# Phase concept derived from (https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code)\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "\n",
        "  for phase in [ 'val']:\n",
        "\n",
        "    if phase == 'train':\n",
        "      print('Running training step...\\n')\n",
        "    else:\n",
        "      print('Running validation...\\n')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc  = 0\n",
        "\n",
        "    if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "    else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "print('\\nPerforming Evaluation on Test Set.')\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_dataloader = test_dataloader if config['split_train'] else val_dataloader\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  #Unpack\n",
        "  input_ids      = batch.get('input_ids').to(device)\n",
        "  attention_mask = batch.get('attention_mask').to(device)\n",
        "  #labels         = batch.get('label').to(device)\n",
        "  tfidf_feats    = None\n",
        "\n",
        "  if ENSEMBLE:\n",
        "    tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "    #transform to tensor - derived from https://github.com/donglinchen/text_classification/blob/master/model_pytorch.ipynb\n",
        "    tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "  # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "  avg_feats = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      #labels=labels,\n",
        "                      ensemble=ENSEMBLE,\n",
        "                      tfidf_feats=tfidf_feats,\n",
        "                      avg_feats=avg_feats)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('   DONE!')\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Write to CSV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# The predictions for each batch are a 2-column ndarray (one column for \"0\"\n",
        "# and one column for \"1\"). With rows refelcting the weight\n",
        "# of the predicition for each label. Identify the highest value and covert\n",
        "# in to a list of 0s and 1s.\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  y_preds.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "# Y_preds stores an array for each batch in our predicitions\n",
        "# to produce a classification report, extract each item in these sublists to a\n",
        "# flattend array.\n",
        "#flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_predictions = [item for sublist in y_preds for item in sublist]\n",
        "\n",
        "df = pd.DataFrame(flat_predictions,columns =['labels'])\n",
        "print(df)\n",
        "#print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Save Model\n",
        "if config['save_model']:\n",
        "\n",
        "  output_dir = './'+config['model']+'_save/'\n",
        "\n",
        "  # Create output directory if needed\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "  # Pytorch reccomends saving models using save/load state dict\n",
        "  # see (https://pytorch.org/tutorials/beginner/saving_loading_models.html#:~:text=Save%2FLoad%20state_dict%20(Recommended)&text=Saving%20the%20model's%20state_dict%20with,pt%20or%20.)\n",
        "  torch.save(model.state_dict(), output_dir + '_model_state.pt')\n",
        "  torch.save(model, output_dir + 'model.pt')\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Memory optimization\n",
        "del model, optimizer, schedular\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "torch.cuda.empty_cache()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Running validation...\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Running validation...\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Running validation...\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Running validation...\n",
            "\n",
            "\n",
            "Performing Evaluation on Test Set.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   DONE!\n",
            "   labels\n",
            "0       1\n",
            "1       1\n",
            "2       1\n",
            "3       1\n",
            "4       1\n",
            "5       0\n",
            "6       1\n",
            "7       1\n",
            "8       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation"
      ],
      "metadata": {
        "id": "73wb-J7usHu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils import data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import ( AdamW, BertModel, RobertaModel)\n",
        "from transformers import BertTokenizer, RobertaTokenizer\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import scipy\n",
        "import nltk\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import re\n",
        "import csv\n",
        "import argparse\n",
        "import gc\n",
        "import sys\n",
        "import os\n",
        "from nltk import word_tokenize\n"
      ],
      "metadata": {
        "id": "B-uWEiQL6kh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "# Stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# Set GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpqX4gmr63DE",
        "outputId": "610fd912-33ba-4c20-a699-ebb8d6288a1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================================================================================\n",
        "# Functions\n",
        "\n",
        "def string_to_bool(string):\n",
        "\n",
        "    if string.lower() == 'false':\n",
        "        return False\n",
        "    elif string.lower() == 'true':\n",
        "        return True\n",
        "    else:\n",
        "        return ValueError\n",
        "\n",
        "\n",
        "def pre_process(tweet):\n",
        "\n",
        "    tweet = re.sub(\"HTTPURL\", \"http\", tweet)\n",
        "    tweet = \" \".join([\" \".join([word for word in re.sub('([A-Z][a-z]+)', r' \\1',\n",
        "                                                        re.sub('([A-Z]+)', r' \\1', word)).split()]) if word.startswith(\n",
        "        '#') else word for word in tweet.split()])\n",
        "    tweet = re.sub('# ', '#', tweet)\n",
        "\n",
        "    # convert instances of covid/corona to 'virus' for which there is an embedding\n",
        "    tweet = re.sub('(?i)CORONA$', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID-19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVIDー19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirus_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)COVID_19', 'coronavirus', tweet)\n",
        "    tweet = re.sub('(?i)coronavirusー19', 'coronavirus', tweet)\n",
        "\n",
        "    return tweet\n",
        "\n",
        "def remove_emojis(data):\n",
        "\n",
        "    emoj = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U00002702-\\U000027B0\"\n",
        "        u\"\\U000024C2-\\U0001F251\"\n",
        "        u\"\\U0001f926-\\U0001f937\"\n",
        "        u\"\\U00010000-\\U0010ffff\"\n",
        "        u\"\\u2640-\\u2642\"\n",
        "        u\"\\u2600-\\u2B55\"\n",
        "        u\"\\u200d\"\n",
        "        u\"\\u23cf\"\n",
        "        u\"\\u23e9\"\n",
        "        u\"\\u231a\"\n",
        "        u\"\\ufe0f\"  # dingbats\n",
        "        u\"\\u3030\"\n",
        "                      \"]+\", re.UNICODE)\n",
        "    return re.sub(emoj, '', data)\n",
        "\n",
        "\n",
        "def handcrafted_feature(text):\n",
        "\n",
        "    text = remove_emojis(text)\n",
        "    text = [char for char in text if char not in string.punctuation]\n",
        "    count = len([char for char in text if char.isdigit()])\n",
        "    avg = (count/(len(text) - text.count(\" \")))*100\n",
        "    x =  torch.tensor([avg], dtype=torch.float)\n",
        "    y = x.view(1,1)\n",
        "    return y\n",
        "\n",
        "\n",
        "config     = {\"model_class\": \"RoBERTa\",\n",
        "\"model\" : \"roberta-base\",\n",
        "\"random_seed\": int(\"4\"),\n",
        "\"max_len\": int(\"128\"),\n",
        "\"epochs\": int(\"4\"),\n",
        "\"learning_rate\": float(\"1e-5\"),\n",
        "\"batch_size\": int(\"32\"),\n",
        "\"dropout_prob\": float(\"0.2\"),\n",
        "\"test_size\": float(\"0.3\"),\n",
        "\"preprocessed\": string_to_bool(\"True\"),\n",
        "\"ensemble\": string_to_bool(\"True\"),\n",
        "\"num_labels\": int(\"2\"),\n",
        "\"max_feats\": int(\"6000\"),\n",
        "\"save_model\": string_to_bool(\"False\"),\n",
        "\"split_train\": string_to_bool(\"False\")}\n",
        "\n",
        "\n",
        "\n",
        "train_data = None\n",
        "df_val     = None\n",
        "\n",
        "MODEL         = None\n",
        "RANDOM_SEED   = None\n",
        "MAX_LEN       = None\n",
        "BATCH_SIZE    = None\n",
        "EPOCHS        = None\n",
        "LEARNING_RATE = None\n",
        "TEST_SIZE     = None\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# read in data\n",
        "\n",
        "try:\n",
        "  train_data = pd.read_csv('train.tsv', sep=r'\\t', header=0)\n",
        "  #assert train_data.shape == (69, 3) , \"train.tsv should contain 7000 rows, only found \" + str(train_data.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/train.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "try:\n",
        "  df_val = pd.read_csv('valid.tsv', sep=r'\\t', header=0)\n",
        "\n",
        "  #assert df_val.shape == (9, 3) , \"valid.tsv should contain 1000 rows, only found \" + str(df_val.shape[0])\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file ' + './datasets/valid.tsv' + ' ' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n",
        "\n",
        "\n",
        "# Apply preprocessing to train text and validation text\n",
        "if config['preprocessed']:\n",
        "  train_data.Text = train_data.Text.apply(lambda x: pre_process(x))\n",
        "  #print(df_val)\n",
        "  df_val.Text     = df_val.Text.apply(lambda x: pre_process(x))\n",
        "\n",
        "# Currently the labels in our dataframes are \"UNINFORMATIVE\" or \"INFORMATIVE\"\n",
        "# It is therefore neccessary to convert these to binary values for classification\n",
        "train_data.Label  = train_data.Label.apply(lambda x: 0 if x == 'UNINFORMATIVE' else 1)\n",
        "df_val.Label      = df_val.Label.apply(lambda x: 0 if x=='UNINFORMATIVE' else 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQXt6c0s66Yp",
        "outputId": "f6d0c475-c28b-4648-a891-10ed76418474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:112: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================================================================================\n",
        "# Configure Model\n",
        "\n",
        "TRANSFORMERS = {\n",
        "    \"BERT\" : (BertModel, BertTokenizer),\n",
        "    \"RoBERTa\": (RobertaModel, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "transformer_class, tokenizer_class = TRANSFORMERS.get(config[\"model_class\"])\n",
        "\n",
        "# Define model\n",
        "transformer_model = transformer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define tokenizer\n",
        "tokenizer   = tokenizer_class.from_pretrained(config[\"model\"])\n",
        "\n",
        "# Define sequence length, batch size and random state\n",
        "MAX_LEN     = config['max_len']\n",
        "BATCH_SIZE  = config['batch_size']\n",
        "RANDOM_SEED = config['random_seed']\n",
        "TEST_SIZE   = config['test_size']\n",
        "ENSEMBLE    = config['ensemble']\n",
        "MAX_FEATS   = config['max_feats']\n",
        "\n",
        "if RANDOM_SEED is not None:\n",
        "  # Propogate random_state\n",
        "  np.random.seed(RANDOM_SEED)\n",
        "  torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "#transformer_model.config.model_type\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "0b2d6cf479d542a28f332e2978b8204b",
            "d5d118c76797450ea11dcd9459c0088e",
            "8bd2e4ff5ec54927a550dc44285a33a1",
            "1abff20776ea40b9b3f1a31f3c8447f3",
            "91d8098897a74df2861dbedf76f178a4",
            "f80772bdf0294930872eea2f521736e7",
            "5238b219fe51482d81f6b8a86921fbea",
            "d54b4a7c189b4a1397a177bfe822d1f3",
            "5918af03bf384cba856ab3bfc5139082",
            "0d044f310d754195ab83809dd6c3763c",
            "eb54ffe8f5d64a68a1b94ef7c036d948",
            "39830252c4e84c849e3e9ff105ad772f",
            "8a265553f4c14df7b9d9b7f33e9ebc76",
            "f5732462263748cf8e8c523ba71e3bd3",
            "5bf4574dc5f24e9793a78d70ce57bbd2",
            "077c4e85d0e743769e7811b84a6f8ef7",
            "bd4f9f8b275142249efbec848f713098",
            "fb2b87ff223b492db3551751582d8709",
            "a1d1f76bd98642d5bc6d9c9ad4a01ba0",
            "4baeba2677be4152ba2d8ae30c6b4253",
            "1334a00873b84796a4d28f1d8aab2a43",
            "babd26e8000f4b019e297f8792ebf508",
            "250adc1f5f194dbd96065cfa869ed1f8",
            "4d5cda16398c421a9a22eaf9bc5ed60e",
            "1f451c0afbcf4ee9ad4af100d0f168e7",
            "7c2dbdabdac144ce99a26b531f799731",
            "0009900c89bd41dcbe921829c6507fcf",
            "f86f6c1cce2540f0aeb489106752147d",
            "1f904835d034472497903b741a49ec62",
            "528e1a2e18f9479baf0460fbed668e6c",
            "4c53223dbc284289a18d498db9d16e2d",
            "32cd0cef48444803b49b6680aff760f0",
            "c9e58a6cf64f480ebdd1a18127cab5e0",
            "a43b5fa66321439eaa7a051205b51c93",
            "d5e6e12511f74ef8ad7772eb96d35439",
            "a82beeb6d6804d22902ea83bdd12e8f7",
            "d3783d328fe145e38059c418456e0f58",
            "b0e1cf3a1c5b479ea67aed5176548601",
            "671bb89065ad42029641079f604bc810",
            "7b22dec0da5a4c4c8e7d56a1b7919128",
            "12145d14186945999214a675d2c3ab3d",
            "788f916ba0d144dd80258c48d4c36134",
            "607cd98f58934ef082cd916c8ea41ea1",
            "47e6636aa6634ac881f19fb872a9eb55",
            "dea944d3b6b9439991e891fc0b32f351",
            "c271517570fa4070b988e48d4a4ef3ce",
            "27395344c2704155871f94d991bebabb",
            "30dffb37cf6f4d3a878a6027ddcc75d5",
            "1d46e12d5c044b2382116676362c39f6",
            "b423ad1dfd6a4c02a15a075ba5925794",
            "d6dfca3d6102450db5bcef1e58b88b9f",
            "971e0f9f76a04978a8d459bfece996cc",
            "a5503127e86544c88c7899588d6e2477",
            "fffe234d6ad646209cfa470e681f441b",
            "b79b936152054537ae04d0d228395790"
          ]
        },
        "id": "hd-KNUzX8A4A",
        "outputId": "869b8bd1-73dc-47d3-fb0e-6fddb59bea8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0b2d6cf479d542a28f332e2978b8204b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39830252c4e84c849e3e9ff105ad772f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "250adc1f5f194dbd96065cfa869ed1f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a43b5fa66321439eaa7a051205b51c93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dea944d3b6b9439991e891fc0b32f351",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Class derived from (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "class tweet_dataset(data.Dataset):\n",
        "\n",
        "  def __init__(self, id, text, label, tokenizer, max_len):\n",
        "    self.id = id\n",
        "    self.text = text\n",
        "    self.label = label\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    text = str(self.text[item])\n",
        "\n",
        "    encoding = tokenizer.encode_plus(\n",
        "      text,\n",
        "      max_length=self.max_len,\n",
        "      truncation=True,\n",
        "      add_special_tokens=True,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_token_type_ids=False,\n",
        "      return_tensors='pt' # Return pytorch tenosors\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'text' : text,\n",
        "        'input_ids' : encoding['input_ids'].flatten(), # flatten to get right shape\n",
        "        'attention_mask' : encoding['attention_mask'].flatten(), # flatten to get right shape\n",
        "        'label' : torch.tensor(self.label[item], dtype=torch.long),\n",
        "        'id' : torch.tensor(self.id[item], dtype=torch.long)\n",
        "    }\n",
        "\n",
        "# Note -- this is the same implementation used in BERT_baseline\n",
        "\n",
        "# Wrap these datasets into pytorch dataloaders\n",
        "# Credit for this method goes to (https://www.curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/)\n",
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  #Create instance of dataset\n",
        "  ds = tweet_dataset(\n",
        "      id = df.Id.to_numpy(),\n",
        "      text = df.Text.to_numpy(),\n",
        "      label = df.Label.to_numpy(),\n",
        "      tokenizer = tokenizer,\n",
        "      max_len = max_len\n",
        "  )\n",
        "  return data.DataLoader(\n",
        "      ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=4\n",
        "  )\n",
        "\n",
        "if config['split_train']: # split training to create a test set\n",
        "  df_train, df_test = train_test_split(train_data, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
        "  test_dataloader   = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# Create loaders for training, test and val data\n",
        "train_dataloader = (create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE) if config['split_train']\n",
        "                    else create_data_loader(train_data, tokenizer, MAX_LEN, BATCH_SIZE))\n",
        "val_dataloader   = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# TF-IDF MODEL\n",
        "\n",
        "# Define a method that will passed as an analyzer to the TfidfVectorizer\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Method to clean the tweet text, removing case, removing stopwords and punctuation and tokenizing.\n",
        "    This will be parsed to the TfidfVectoriser as an analyzer and applied to each tweet in fitting.\n",
        "    :param text: The text of each tweet\n",
        "    :return: The text tokenized, with punctuation and stopwords removed.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = remove_emojis(text)\n",
        "    text = \"\".join([char for char in text if not char.isdigit()])\n",
        "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
        "    return text\n",
        "\n",
        "if ENSEMBLE:\n",
        "\n",
        "  # Instantiate TfidfVectorizer object and pass clean_text method which it will automatically apply\n",
        "  tfidf_vect = TfidfVectorizer(analyzer=clean_text, max_features=MAX_FEATS)\n",
        "\n",
        "  # Learning vocabulary on training data\n",
        "  if config['split_train']:\n",
        "        tfidf_vect.fit(df_train.Text)\n",
        "  else:\n",
        "        tfidf_vect.fit(train_data.Text)\n",
        "\n",
        "# =="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zhvwktk6_TG",
        "outputId": "e394e638-39f3-4863-8ba0-72cf33762a49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RobertaClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(RobertaClassificationHead, self).__init__()\n",
        "    self.input_size  = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels  = num_labels\n",
        "    self.dropout     = nn.Dropout(p=dropout_prob)\n",
        "    self.activation  = nn.Tanh()\n",
        "    #self.dense      = nn.Linear(hidden_size , hidden_size) # Note - atm hidden_size\n",
        "    self.dense      = nn.Linear(self.input_size , self.input_size) # Note - atm hidden_size\n",
        "    self.out_proj   = nn.Linear(self.input_size , self.num_labels)\n",
        "\n",
        "  # Derived from (https://huggingface.co/transformers/v1.1.0/_modules/pytorch_transformers/modeling_roberta.html)\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dropout(x) # Roberta has extra dropout layer, and experimentation showed this gets better results.\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class BertClassificationHead(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, num_labels, num_tfidf_feats, dropout_prob):\n",
        "    super(BertClassificationHead, self).__init__()\n",
        "    self.input_size = hidden_size + num_tfidf_feats + 1 #plus 1 to account for avg\n",
        "    self.num_labels = num_labels\n",
        "    self.activation = nn.Tanh()\n",
        "    self.dense      = nn.Linear(self.input_size, self.input_size)\n",
        "    self.dropout    = nn.Dropout(p=dropout_prob)\n",
        "    self.out_proj   = nn.Linear(self.input_size, self.num_labels)\n",
        "\n",
        "  # Dervied from (https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertModel)\n",
        "  def forward(self, features, ensemble, tfidf_feats, avg_feats):\n",
        "\n",
        "    x = features[:, 0]  # take [CLS]\n",
        "\n",
        "    if ensemble:\n",
        "    # Concatenate tfidf_feature vector to RoBERTa <s> token embedding\n",
        "      x = torch.cat((x, tfidf_feats), dim=1)\n",
        "\n",
        "    x = torch.cat((x, avg_feats), dim=1)  # adding the extra avg_num features\n",
        "    x = self.dense(x)\n",
        "    x = self.activation(x)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "    x = self.out_proj(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self, model, num_labels, classification_head):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.num_labels = num_labels\n",
        "    self.classification_head = classification_head\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, labels=None, ensemble=None, tfidf_feats=None, avg_feats=None):\n",
        "\n",
        "    outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # if self.model.config.model_type == 'bert':\n",
        "    #   # 'outputs' is a tuple (sequence_output, pooled_output).\n",
        "    #   # The pooled_output is the last hidden-state of the [CLS] token further process by a Linear layer and tanh function\n",
        "    #   model_output = outputs[1]\n",
        "\n",
        "    # else:\n",
        "    #   # 'outputs' is raw hidden states from RoBERTa\n",
        "    #   # Therefore take the sequence output of dimensions (batch_size, max_len, hidden_size)\n",
        "    #   model_output = outputs[0]\n",
        "\n",
        "    # Need to avoid using pooled output from BERT (where making use of tfidf)\n",
        "    # therefore in each case BERT and RoBERTa going to use the sequence output\n",
        "    # and in the classification head extract the CLS token or equivalent\n",
        "\n",
        "    model_output = outputs[0]\n",
        "\n",
        "    logits = self.classification_head(model_output, ensemble, tfidf_feats, avg_feats)\n",
        "\n",
        "    outputs = (logits,)\n",
        "\n",
        "    # Following derived from (https://huggingface.co/transformers/_modules/transformers/modeling_bert.html#BertForSequenceClassification)\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "    outputs = (loss,) + outputs\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "ULbAZuIC8Rfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================================================================================\n",
        "# Create instance of model and classificationHead\n",
        "hidden_size     = transformer_model.config.hidden_size\n",
        "num_labels      = config[\"num_labels\"]\n",
        "num_tfidf_feats = len(tfidf_vect.get_feature_names()) if config[\"ensemble\"] else 0\n",
        "dropout_prob    = config[\"dropout_prob\"]\n",
        "\n",
        "# Two different classification heads - distinct by a dropout layer\n",
        "classification_head = (BertClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob) if config[\"model_class\"] == \"BERT\"\n",
        "                      else RobertaClassificationHead(hidden_size, num_labels, num_tfidf_feats, dropout_prob))\n",
        "\n",
        "model = Classifier(transformer_model, num_labels, classification_head)\n",
        "\n",
        "# Move to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "LEARNING_RATE    = config[\"learning_rate\"]\n",
        "EPOCHS           = config[\"epochs\"]\n",
        "\n",
        "WEIGHT_DECAY     = 0 # defaults to 0 in AdamW\n",
        "NUM_WARMUP_STEPS = 0\n",
        "TOTAL_STEPS      = len(train_dataloader) * EPOCHS\n",
        "\n",
        "if config[\"model_class\"] == \"RoBERTa\":\n",
        "  WEIGHT_DECAY     = 0.1\n",
        "  NUM_WARMUP_STEPS = int(0.06 * TOTAL_STEPS) # 0.06 ratio\n",
        "\n",
        "# Use a pytorch optimizer to preform updates on parameters.\n",
        "# This is the Adam algorithm with weight decay fix. This is the same optimizer\n",
        "# implemented in (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "# and the hugginface implementation of transformer models on the GLUE dataset,\n",
        "# see (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False, weight_decay=WEIGHT_DECAY) # BERT tf library use false.\n",
        "\n",
        "# Create a schedular which will update the learning rate.\n",
        "# This uses the defualt num_warmup_steps from:\n",
        "# (https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L392)\n",
        "schedular = get_linear_schedule_with_warmup(\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "    num_training_steps=TOTAL_STEPS\n",
        ")\n",
        "\n",
        "# Function to calculate the accuracy of a batch of predictions vs true labels\n",
        "# Function is derived from (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten() # returns max values on columns\n",
        "    labels_flat = labels.flatten()\n",
        "    assert len(pred_flat) == len(labels_flat)\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "\n",
        "# ==============================================================================================================================================\n",
        "# Training Loop\n",
        "\n",
        "# The below code forms the fine-tuning training loop of our model on the new data.\n",
        "# This code was influenced by (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk).\n",
        "# With ammendments to reflect the change that our dataset returns a dictionary, and the introduction of a phase for loop to reduce visual complexity.\n",
        "\n",
        "# As in (https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=8o-VEBobKwHk)\n",
        "# we will store the losses associated with training and validiation.\n",
        "history = defaultdict(list)\n",
        "\n",
        "# define a dict with our dataloaders\n",
        "dataloader = {\n",
        "    'train' : train_dataloader,\n",
        "    'val'   : val_dataloader\n",
        "}\n",
        "\n",
        "# Phase concept derived from (https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code)\n",
        "for epoch in range(0, EPOCHS):\n",
        "\n",
        "  print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))\n",
        "\n",
        "  for phase in ['train', 'val']:\n",
        "\n",
        "    if phase == 'train':\n",
        "      print('Running Training step...\\n')\n",
        "    else:\n",
        "      print('Running Testing step...\\n')\n",
        "\n",
        "    total_loss = 0\n",
        "    total_acc  = 0\n",
        "\n",
        "    if phase == 'train':\n",
        "        model.train()  # Set model to training mode\n",
        "    else:\n",
        "        model.eval()   # Set model to evaluate mode\n",
        "\n",
        "\n",
        "    for batch in dataloader[phase]:\n",
        "\n",
        "      #Unpack batch\n",
        "      input_ids      = batch.get('input_ids').to(device)\n",
        "      attention_mask = batch.get('attention_mask').to(device)\n",
        "      labels         = batch.get('label').to(device)\n",
        "      # Declare a none-type variable where model not ensemble - only concatenated where ensemble == True\n",
        "      tfidf_feats = None\n",
        "\n",
        "      if ENSEMBLE:\n",
        "        tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "        #transform to tensor - derived from https://github.com/donglinchen/text_classification/blob/master/model_pytorch.ipynb\n",
        "        tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "      # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "      avg_feats      = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "      # Pytorch accumulates the gradients on backward pass so need to set them to zero.\n",
        "      model.zero_grad()\n",
        "\n",
        "      with torch.set_grad_enabled(phase == 'train'):\n",
        "        # Perform a forward pass on this training batch.\n",
        "        # As explained in (https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification),\n",
        "        # because we pass the true labels, the classification loss is returned along with the classification\n",
        "        # scores before normalizaition (\"logits\")\n",
        "        loss, logits = model(input_ids=input_ids,\n",
        "                              attention_mask=attention_mask,\n",
        "                              labels=labels,\n",
        "                              ensemble=ENSEMBLE,\n",
        "                              tfidf_feats=tfidf_feats,\n",
        "                              avg_feats=avg_feats) # Default is None, but need to declare a var for when ensemble True\n",
        "\n",
        "        # Accumulate the training loss to calculate average loss for this epoch.\n",
        "        # loss.item() extracts the value from the loss tensor as a python float.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        total_acc += flat_accuracy(logits, label_ids)\n",
        "\n",
        "        if phase == 'train':\n",
        "\n",
        "          # Perform a backward pass to calculate the gradients.\n",
        "          loss.backward()\n",
        "\n",
        "          # Use gradient clipping to prevent exploding gradients.\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "          # Update parameters\n",
        "          optimizer.step()\n",
        "\n",
        "          # Update the learning rate.\n",
        "          schedular.step()\n",
        "\n",
        "    avg_acc  = total_acc / len(dataloader[phase])\n",
        "    avg_loss = total_loss / len(dataloader[phase])\n",
        "\n",
        "    if phase == 'train':\n",
        "      print(\"Training step complete. \\n\")\n",
        "    else:\n",
        "      print('Testing step complete. \\n')\n",
        "\n",
        "    print(\"   Average loss: {0:.2f}\".format(avg_loss))\n",
        "    print(\"   Average accuracy {0:.2f}\".format(avg_acc))\n",
        "    print(\"\")\n",
        "\n",
        "    # Record all statistics from this epoch and phase.\n",
        "    history[phase +'_loss'].append(avg_loss)\n",
        "    history[phase +'_acc'].append(avg_acc)\n",
        "\n",
        "\n",
        "print(\"\\nTraining complete!\")\n",
        "\n",
        "#print('\\nPerforming Evaluation on Test Set.')\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "test_dataloader = test_dataloader if config['split_train'] else val_dataloader\n",
        "\n",
        "for batch in test_dataloader:\n",
        "\n",
        "  #Unpack\n",
        "  input_ids      = batch.get('input_ids').to(device)\n",
        "  attention_mask = batch.get('attention_mask').to(device)\n",
        "  labels         = batch.get('label').to(device)\n",
        "  tfidf_feats    = None\n",
        "\n",
        "  if ENSEMBLE:\n",
        "    tfidf_feats    = tfidf_vect.transform(batch.get('text')) #transform text in batch\n",
        "\n",
        "    #transform to tensor - derived from https://github.com/donglinchen/text_classification/blob/master/model_pytorch.ipynb\n",
        "    tfidf_feats    = torch.tensor(scipy.sparse.csr_matrix.todense(tfidf_feats)).float().to(device)\n",
        "\n",
        "  # apply average nums to each text item, and reshape to tensor of [32,1]\n",
        "  avg_feats = torch.stack([handcrafted_feature(text) for text in batch.get('text')])[:, -1, :].to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    outputs = model(input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      labels=labels,\n",
        "                      ensemble=ENSEMBLE,\n",
        "                      tfidf_feats=tfidf_feats,\n",
        "                      avg_feats=avg_feats)\n",
        "\n",
        "    logits = outputs[1]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('   DONE!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BqBb9CqX8XwS",
        "outputId": "2be8a1c3-4f7d-4a48-a30c-27e04049f4f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Running Training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.29\n",
            "   Average accuracy 0.87\n",
            "\n",
            "Running Testing step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing step complete. \n",
            "\n",
            "   Average loss: 0.50\n",
            "   Average accuracy 0.88\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Running Training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.10\n",
            "   Average accuracy 0.97\n",
            "\n",
            "Running Testing step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing step complete. \n",
            "\n",
            "   Average loss: 0.47\n",
            "   Average accuracy 0.89\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Running Training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.05\n",
            "   Average accuracy 0.99\n",
            "\n",
            "Running Testing step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing step complete. \n",
            "\n",
            "   Average loss: 0.56\n",
            "   Average accuracy 0.89\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Running Training step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training step complete. \n",
            "\n",
            "   Average loss: 0.03\n",
            "   Average accuracy 0.99\n",
            "\n",
            "Running Testing step...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing step complete. \n",
            "\n",
            "   Average loss: 0.58\n",
            "   Average accuracy 0.89\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   DONE!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================================================================================\n",
        "# Write to CSV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# The predictions for each batch are a 2-column ndarray (one column for \"0\"\n",
        "# and one column for \"1\"). With rows refelcting the weight\n",
        "# of the predicition for each label. Identify the highest value and covert\n",
        "# in to a list of 0s and 1s.\n",
        "\n",
        "y_preds = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  y_preds.append(np.argmax(predictions[i], axis=1).flatten())\n",
        "\n",
        "# Y_preds stores an array for each batch in our predicitions\n",
        "# to produce a classification report, extract each item in these sublists to a\n",
        "# flattend array.\n",
        "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
        "flat_predictions = [item for sublist in y_preds for item in sublist]\n",
        "\n",
        "print(classification_report(flat_true_labels, flat_predictions))\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score\n",
        "\n",
        "def compute_metrics(labels, predictions):\n",
        "\n",
        "    # precision and recall only for INFORMATIVE class\n",
        "    precision = precision_score(labels, predictions, pos_label=1, average='binary')\n",
        "    recall    = recall_score(labels, predictions, pos_label=1, average='binary')\n",
        "    # weighted f1-score\n",
        "    f1        = f1_score(labels, predictions, average='weighted')\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, predictions, labels=[0,1]).ravel()\n",
        "\n",
        "    return {\n",
        "        \"precision\" : precision,\n",
        "        \"recall\"    : recall,\n",
        "        \"f1_score\"  : f1,\n",
        "        \"tp\"        : tp,\n",
        "        \"fp\"        : fp,\n",
        "        \"fn\"        : fn,\n",
        "        \"tn\"        : tn\n",
        "    }\n",
        "\n",
        "#Write out to csv file:\n",
        "\n",
        "try:\n",
        "  with open('./transformer_model_tests.csv', 'a', newline='') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "\n",
        "    metrics = compute_metrics(flat_true_labels, flat_predictions)\n",
        "\n",
        "    csv_writer.writerow([config['model_class'], config['model'], config['split_train'],\n",
        "                         config['preprocessed'], config['ensemble'],config['save_model'],\n",
        "                         config['random_seed'],config['max_len'], config['batch_size'],\n",
        "                         config['learning_rate'], config['epochs'], config['dropout_prob'],config['max_feats'],\n",
        "                         history['train_acc'], history['val_acc'], history['train_loss'],history['val_loss'],\n",
        "                         metrics['precision'], metrics['recall'], metrics['f1_score'],\n",
        "                         metrics['tp'], metrics['fp'], metrics['fn'], metrics['tn']])\n",
        "\n",
        "except IOError as e:\n",
        "  sys.stderr.write('Error opening file trandormer_model_tests.csv' + e.strerror + '\\n')\n",
        "  raise SystemExit\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJp_Sbn_8e_W",
        "outputId": "4184331f-58e6-4812-aa43-26965f3a6802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.88      0.89      1056\n",
            "           1       0.87      0.90      0.89       944\n",
            "\n",
            "    accuracy                           0.89      2000\n",
            "   macro avg       0.89      0.89      0.89      2000\n",
            "weighted avg       0.89      0.89      0.89      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the confusion matrix\n",
        "cf_matrix = confusion_matrix(flat_true_labels, flat_predictions)\n",
        "print(cf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEmBLDSlC5rg",
        "outputId": "7fa18efa-b900-4fe5-ed6d-238b36698187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[931 125]\n",
            " [ 94 850]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,\n",
        "            fmt='.2%', cmap='Blues')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "Ra88WlHhDITi",
        "outputId": "e985f71a-4455-440b-86e0-0628cd5aec91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fb6fb194ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(flat_true_labels, flat_predictions)\n",
        "print('Accuracy: %f' % accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXHm7U2UDpnC",
        "outputId": "45282435-6a42-444a-fc3c-9c68bec29b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.890500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(flat_true_labels, flat_predictions)\n",
        "print('Precision: %f' % precision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kulscAZ1DvbX",
        "outputId": "a432d5d8-5243-40a6-d9e7-ad5fd3405552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.871795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall = recall_score(flat_true_labels, flat_predictions)\n",
        "print('Recall: %f' % recall)\n",
        "# f1: 2 tp / (2 tp + fp + fn)\n",
        "f1 = f1_score(flat_true_labels, flat_predictions)\n",
        "print('F1 score: %f' % f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JASx1uCoD1gd",
        "outputId": "25fac225-5441-4a16-8f17-1c4c24c71828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall: 0.900424\n",
            "F1 score: 0.885878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "ma=mape(flat_true_labels, flat_predictions)\n",
        "print('Mean absolute percentage error : ', ma )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUTkQSdtEy0N",
        "outputId": "4a911d7f-dcd4-4cb6-e81f-c8225e1805dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean absolute percentage error :  0.07825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(flat_true_labels)\n",
        "for index,item in enumerate(flat_true_labels):\n",
        "  if item==2:\n",
        "    flat_true_labels[index]=0\n"
      ],
      "metadata": {
        "id": "fUdHd_YGGeAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index,item in enumerate(flat_predictions):\n",
        "  if item==2:\n",
        "    flat_predictions[index]=0"
      ],
      "metadata": {
        "id": "DDlS3gy2HcRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mape(actual, pred):\n",
        "    actual, pred = np.array(actual), np.array(pred)\n",
        "    return np.mean(np.abs((actual - pred) / actual))"
      ],
      "metadata": {
        "id": "8svTJUNqE1l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "fpr, tpr, _ = metrics.roc_curve(flat_true_labels,  flat_predictions)\n",
        "auc = metrics.roc_auc_score(flat_true_labels,flat_predictions)\n",
        "\n",
        "#create ROC curve\n",
        "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "baQaiy9lIiGo",
        "outputId": "a56dc4a4-a831-4fca-81ad-c0cd74601091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cf_matrix"
      ],
      "metadata": {
        "id": "i9n-GvkSJjI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sensitivity1 = cf_matrix[0,0]/(cf_matrix[0,0]+cf_matrix[0,1])\n",
        "print('Sensitivity : ', sensitivity1 )\n",
        "\n",
        "specificity1 = cf_matrix[1,1]/(cf_matrix[1,0]+cf_matrix[1,1])\n",
        "print('Specificity : ', specificity1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WETz-cSSJ48P",
        "outputId": "4de6c1fa-7a1e-4365-bc12-841fba0b434c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity :  0.8816287878787878\n",
            "Specificity :  0.9004237288135594\n"
          ]
        }
      ]
    }
  ]
}